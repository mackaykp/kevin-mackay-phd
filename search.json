[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "I am a data, spatial, and research professional with experience across municipal, federal, and academic public‑sector environments. I support the full lifecycle of analytical and research projects—from problem definition and data quality assessment to analysis, communication, and workflow design.\n\n\n\n\n\nSupported policy and program teams by analyzing data quality, documenting assumptions, and developing QA guidance to strengthen program oversight and evidence‑informed decision‑making.\nContributed to corporate capacity‑building by preparing a business case for analytics and digital skills training, linking emerging tools to long‑term organizational capability and efficiency.\n\n\n\n\n\nAnalyzed administrative and survey data to support program monitoring, performance measurement, and spatial analysis.\nCoordinated workflows in a secure data environment, supporting data access, documentation, and compliance.\nAdvised internal stakeholders on data limitations, methodological trade‑offs, and appropriate interpretation of analytical results.\n\n\n\n\n\nCoordinated research activities across multiple analytics initiatives, supporting planning, task tracking, and cross‑functional collaboration.\nAnalyzed large administrative and spatial datasets to support segmentation, outreach planning, and evidence‑informed service delivery.\nManaged concurrent research requests from intake through delivery, tracking scope, schedules, milestones, and risks.\nSupported ethical and procedural compliance by maintaining documentation and reinforcing standards across projects.\n\n\n\n\n\nDelivered hands‑on training in applied data analysis and spatial data science.\nDeveloped course materials, assignments, and tutorials to support skill development and tool adoption across diverse audiences.\n\n\n\n\n\nConducted spatial and analytical research to support public policy and program analysis.\nLed a Google Street View–based project from design through implementation, producing a public‑facing digital resource with over 8 million views.\n\n\n\n\n\n\nPhD in Geography, McMaster University (2020)\n\nMaster of Spatial Analysis, Toronto Metropolitan University (2013)\n\nHonours Geography & Environmental Studies (Minor in GIS), McMaster University (2012)\n\n\n\n\n\nData & Spatial Analysis — R, Python, SQL, ArcGIS, spatial modelling, administrative data\n\nResearch & Evaluation — mixed methods, evidence synthesis, policy analysis, UX research\n\nData Quality & Governance — validation, documentation, reproducibility, workflow design\n\nProject Coordination — planning, task tracking, stakeholder engagement, cross‑functional collaboration\n\nCommunication & Enablement — dashboards, reporting, training, user guides, presentations"
  },
  {
    "objectID": "resume.html#experience",
    "href": "resume.html#experience",
    "title": "Resume",
    "section": "",
    "text": "Supported policy and program teams by analyzing data quality, documenting assumptions, and developing QA guidance to strengthen program oversight and evidence‑informed decision‑making.\nContributed to corporate capacity‑building by preparing a business case for analytics and digital skills training, linking emerging tools to long‑term organizational capability and efficiency.\n\n\n\n\n\nAnalyzed administrative and survey data to support program monitoring, performance measurement, and spatial analysis.\nCoordinated workflows in a secure data environment, supporting data access, documentation, and compliance.\nAdvised internal stakeholders on data limitations, methodological trade‑offs, and appropriate interpretation of analytical results.\n\n\n\n\n\nCoordinated research activities across multiple analytics initiatives, supporting planning, task tracking, and cross‑functional collaboration.\nAnalyzed large administrative and spatial datasets to support segmentation, outreach planning, and evidence‑informed service delivery.\nManaged concurrent research requests from intake through delivery, tracking scope, schedules, milestones, and risks.\nSupported ethical and procedural compliance by maintaining documentation and reinforcing standards across projects.\n\n\n\n\n\nDelivered hands‑on training in applied data analysis and spatial data science.\nDeveloped course materials, assignments, and tutorials to support skill development and tool adoption across diverse audiences.\n\n\n\n\n\nConducted spatial and analytical research to support public policy and program analysis.\nLed a Google Street View–based project from design through implementation, producing a public‑facing digital resource with over 8 million views."
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Resume",
    "section": "",
    "text": "PhD in Geography, McMaster University (2020)\n\nMaster of Spatial Analysis, Toronto Metropolitan University (2013)\n\nHonours Geography & Environmental Studies (Minor in GIS), McMaster University (2012)"
  },
  {
    "objectID": "resume.html#skills",
    "href": "resume.html#skills",
    "title": "Resume",
    "section": "",
    "text": "Data & Spatial Analysis — R, Python, SQL, ArcGIS, spatial modelling, administrative data\n\nResearch & Evaluation — mixed methods, evidence synthesis, policy analysis, UX research\n\nData Quality & Governance — validation, documentation, reproducibility, workflow design\n\nProject Coordination — planning, task tracking, stakeholder engagement, cross‑functional collaboration\n\nCommunication & Enablement — dashboards, reporting, training, user guides, presentations"
  },
  {
    "objectID": "what-i-do.html",
    "href": "what-i-do.html",
    "title": "What I Do",
    "section": "",
    "text": "I help organizations make sense of complex health, spatial, and administrative data by supporting the full lifecycle of analytical and research projects. My work blends data analysis, spatial methods, research design, documentation, and cross-functional coordination to create solutions that are clear, reliable, and future‑proof.\n\n\nI work with health, housing, transportation, and administrative datasets to uncover patterns, identify risks, and support evidence‑informed decision-making. My background in medical and spatial geography allows me to integrate GIS, spatial modelling, and statistical analysis to answer real-world questions.\n\n\n\nspatial modelling and cluster analysis\n\nhealth indicators and administrative data analysis\n\nmapping, geocoding, and spatial data integration\n\n\n\n\n\nI design and conduct mixed-methods research that integrates qualitative and quantitative insights. I specialize in translating complex findings into clear, actionable recommendations for policy, programs, and service delivery.\n\n\n\nmixed-methods studies and UX research\n\nenvironmental and jurisdictional scans\n\npolicy-oriented analysis and evidence synthesis\n\n\n\n\n\nI help teams build trust in their data by validating quality, documenting assumptions, and establishing reproducible workflows. I focus on clarity, transparency, and long-term maintainability.\n\n\n\ndata validation and QA frameworks\n\ndocumentation, SOPs, and standards\n\nreproducible pipelines and workflow design\n\n\n\n\n\nI support cross-functional teams by coordinating project activities, clarifying requirements, tracking progress, and ensuring alignment across stakeholders. I bridge the gap between technical and non-technical audiences.\n\n\n\nproject planning and task tracking\n\nstakeholder coordination and communication\n\nmanaging concurrent research and analytics initiatives\n\n\n\n\n\nI translate complex analysis into clear, user-focused outputs and help teams adopt tools and practices that improve efficiency and understanding. I enjoy teaching, documentation, and building capacity.\n\n\n\ndashboards, reports, and presentations\n\ntraining, workshops, and user guides\n\nsupporting adoption of analytical and digital tools\n\n\n\n\n\nVisit the Projects page to see examples of spatial analysis, automation, research, and data-driven work across different domains."
  },
  {
    "objectID": "what-i-do.html#health-spatial-data-analysis",
    "href": "what-i-do.html#health-spatial-data-analysis",
    "title": "What I Do",
    "section": "",
    "text": "I work with health, housing, transportation, and administrative datasets to uncover patterns, identify risks, and support evidence‑informed decision-making. My background in medical and spatial geography allows me to integrate GIS, spatial modelling, and statistical analysis to answer real-world questions.\n\n\n\nspatial modelling and cluster analysis\n\nhealth indicators and administrative data analysis\n\nmapping, geocoding, and spatial data integration"
  },
  {
    "objectID": "what-i-do.html#research-design-evidence-synthesis",
    "href": "what-i-do.html#research-design-evidence-synthesis",
    "title": "What I Do",
    "section": "",
    "text": "I design and conduct mixed-methods research that integrates qualitative and quantitative insights. I specialize in translating complex findings into clear, actionable recommendations for policy, programs, and service delivery.\n\n\n\nmixed-methods studies and UX research\n\nenvironmental and jurisdictional scans\n\npolicy-oriented analysis and evidence synthesis"
  },
  {
    "objectID": "what-i-do.html#data-quality-governance-documentation",
    "href": "what-i-do.html#data-quality-governance-documentation",
    "title": "What I Do",
    "section": "",
    "text": "I help teams build trust in their data by validating quality, documenting assumptions, and establishing reproducible workflows. I focus on clarity, transparency, and long-term maintainability.\n\n\n\ndata validation and QA frameworks\n\ndocumentation, SOPs, and standards\n\nreproducible pipelines and workflow design"
  },
  {
    "objectID": "what-i-do.html#project-coordination-collaboration",
    "href": "what-i-do.html#project-coordination-collaboration",
    "title": "What I Do",
    "section": "",
    "text": "I support cross-functional teams by coordinating project activities, clarifying requirements, tracking progress, and ensuring alignment across stakeholders. I bridge the gap between technical and non-technical audiences.\n\n\n\nproject planning and task tracking\n\nstakeholder coordination and communication\n\nmanaging concurrent research and analytics initiatives"
  },
  {
    "objectID": "what-i-do.html#communication-training-digital-enablement",
    "href": "what-i-do.html#communication-training-digital-enablement",
    "title": "What I Do",
    "section": "",
    "text": "I translate complex analysis into clear, user-focused outputs and help teams adopt tools and practices that improve efficiency and understanding. I enjoy teaching, documentation, and building capacity.\n\n\n\ndashboards, reports, and presentations\n\ntraining, workshops, and user guides\n\nsupporting adoption of analytical and digital tools"
  },
  {
    "objectID": "what-i-do.html#explore-my-work",
    "href": "what-i-do.html#explore-my-work",
    "title": "What I Do",
    "section": "",
    "text": "Visit the Projects page to see examples of spatial analysis, automation, research, and data-driven work across different domains."
  },
  {
    "objectID": "work-with-me.html",
    "href": "work-with-me.html",
    "title": "Work With Me",
    "section": "",
    "text": "I help organizations make sense of complex health, spatial, and administrative data by supporting the full lifecycle of analytical and research projects. Whether you need help defining a problem, improving data quality, conducting analysis, or communicating results, I bring structure, clarity, and rigor to every stage of the work.\n\n\n\n\nI work across the entire analytics and research lifecycle—from requirements gathering and data validation to analysis, interpretation, and communication. I help teams move from uncertainty to clarity with solutions that are reliable, transparent, and future‑proof.\n\n\n\nI help teams clarify questions, identify constraints, and translate ideas into actionable analytical plans. This includes understanding stakeholder needs, assessing feasibility, and outlining the steps required to deliver meaningful results.\n\n\n\nI specialize in validating data, documenting assumptions, and building reproducible workflows. My focus is on creating systems that are maintainable, trustworthy, and aligned with governance requirements.\n\n\n\nI conduct spatial, statistical, and mixed‑methods analysis to support program monitoring, performance measurement, and policy‑oriented decision‑making. I translate complex findings into clear, actionable insights for technical and non‑technical audiences.\n\n\n\nI create dashboards, reports, presentations, and user guides that help teams understand and use their data effectively. I also deliver training and workshops to support adoption of analytical tools and best practices.\n\n\n\n\n\nPublic‑sector teams needing analytical support, research capacity, or workflow improvements\n\nOrganizations looking to improve data quality, documentation, or reproducibility\n\nTeams seeking help with spatial analysis, health data, or administrative datasets\n\nLeaders who need clear, decision‑ready insights grounded in rigorous analysis\n\nClients who want structured, thoughtful support on complex or interdisciplinary problems\n\n\n\n\n\nShort‑term analytical or research support\n\nProject‑based consulting\n\nWorkflow and documentation improvements\n\nTraining, workshops, and digital enablement\n\nPart‑time or full‑time employment opportunities\n\n\n\n\nIf you’d like to discuss a project, collaboration, or role, feel free to reach out:\nEmail: mackaykp@gmail.com"
  },
  {
    "objectID": "work-with-me.html#how-i-can-help",
    "href": "work-with-me.html#how-i-can-help",
    "title": "Work With Me",
    "section": "",
    "text": "I work across the entire analytics and research lifecycle—from requirements gathering and data validation to analysis, interpretation, and communication. I help teams move from uncertainty to clarity with solutions that are reliable, transparent, and future‑proof.\n\n\n\nI help teams clarify questions, identify constraints, and translate ideas into actionable analytical plans. This includes understanding stakeholder needs, assessing feasibility, and outlining the steps required to deliver meaningful results.\n\n\n\nI specialize in validating data, documenting assumptions, and building reproducible workflows. My focus is on creating systems that are maintainable, trustworthy, and aligned with governance requirements.\n\n\n\nI conduct spatial, statistical, and mixed‑methods analysis to support program monitoring, performance measurement, and policy‑oriented decision‑making. I translate complex findings into clear, actionable insights for technical and non‑technical audiences.\n\n\n\nI create dashboards, reports, presentations, and user guides that help teams understand and use their data effectively. I also deliver training and workshops to support adoption of analytical tools and best practices."
  },
  {
    "objectID": "work-with-me.html#who-i-work-with",
    "href": "work-with-me.html#who-i-work-with",
    "title": "Work With Me",
    "section": "",
    "text": "Public‑sector teams needing analytical support, research capacity, or workflow improvements\n\nOrganizations looking to improve data quality, documentation, or reproducibility\n\nTeams seeking help with spatial analysis, health data, or administrative datasets\n\nLeaders who need clear, decision‑ready insights grounded in rigorous analysis\n\nClients who want structured, thoughtful support on complex or interdisciplinary problems"
  },
  {
    "objectID": "work-with-me.html#engagement-options",
    "href": "work-with-me.html#engagement-options",
    "title": "Work With Me",
    "section": "",
    "text": "Short‑term analytical or research support\n\nProject‑based consulting\n\nWorkflow and documentation improvements\n\nTraining, workshops, and digital enablement\n\nPart‑time or full‑time employment opportunities"
  },
  {
    "objectID": "work-with-me.html#lets-connect",
    "href": "work-with-me.html#lets-connect",
    "title": "Work With Me",
    "section": "",
    "text": "If you’d like to discuss a project, collaboration, or role, feel free to reach out:\nEmail: mackaykp@gmail.com"
  },
  {
    "objectID": "projects/example-automation.html",
    "href": "projects/example-automation.html",
    "title": "Example Automation Project",
    "section": "",
    "text": "Placeholder: Describe one automation project — what it does, how it runs, and what it produces.\nExample topics: daily report generation, data pulls from an API, or file processing pipelines."
  },
  {
    "objectID": "projects/example-automation.html#overview",
    "href": "projects/example-automation.html#overview",
    "title": "Example Automation Project",
    "section": "",
    "text": "Placeholder: Describe one automation project — what it does, how it runs, and what it produces.\nExample topics: daily report generation, data pulls from an API, or file processing pipelines."
  },
  {
    "objectID": "projects/example-automation.html#tools",
    "href": "projects/example-automation.html#tools",
    "title": "Example Automation Project",
    "section": "Tools",
    "text": "Tools\n\nR or Python scripts\nScheduling (e.g., cron, GitHub Actions)\nQuarto or R Markdown for reports"
  },
  {
    "objectID": "projects/example-automation.html#takeaways",
    "href": "projects/example-automation.html#takeaways",
    "title": "Example Automation Project",
    "section": "Takeaways",
    "text": "Takeaways\nPlaceholder: What you automated and what you’d do differently next time."
  },
  {
    "objectID": "projects/yoga-video-catalog/index.html",
    "href": "projects/yoga-video-catalog/index.html",
    "title": "Yoga Video Catalog",
    "section": "",
    "text": "Final product\n\n\nProject Overview\n\n\nScripts & Code"
  },
  {
    "objectID": "projects/yoga-video-catalog/index.html#overview",
    "href": "projects/yoga-video-catalog/index.html#overview",
    "title": "Yoga Video Catalog",
    "section": "Overview",
    "text": "Overview\nThis project is a filterable yoga video catalog built from YouTube channel data. It demonstrates an end-to-end data workflow: web scraping, automation, ETL (extract–transform–load), and an interactive front end. The goal was to collect videos from chosen channels, enrich them with metadata (duration, upload date, focus areas), and present a single place to filter by duration, focus, channel, and keyword and to build a personal queue.\nFinal product: The Browse catalog page is the main deliverable — a full-screen filterable catalog with duration/year sliders, focus and channel chips, keyword search, and a “My List” queue (stored in the browser)."
  },
  {
    "objectID": "projects/yoga-video-catalog/index.html#pipeline-overview",
    "href": "projects/yoga-video-catalog/index.html#pipeline-overview",
    "title": "Yoga Video Catalog",
    "section": "Pipeline overview",
    "text": "Pipeline overview\nThe pipeline runs in the source project (see “Project files” below). High-level flow:\n\n\n\n\n\n\n\nStep\nWhat it does\n\n\n\n\n1. Configure\nChannel list lives in data/channel_urls.json (display name → channel Videos page URL).\n\n\n2. Scrape\nscrape_channel.py uses Playwright to open each channel’s Videos page, scroll to load all items, and extract video IDs, titles, and durations into a CSV.\n\n\n3. Enrich\nenrich_videos.py uses yt-dlp (no YouTube API key) to fetch metadata (title, description, duration, upload date, view count, etc.) and derives a focus column from title/description using a keyword map.\n\n\n4. Categorize\ncategorize_data.py adds derived columns: duration buckets, upload year, and view-count bands.\n\n\n5. Focus\nupdate_focus.py refreshes the focus column using the same keyword logic (e.g. after config changes).\n\n\n6. Update (orchestrator)\nupdate_from_channels.py runs the full flow: for each channel, scrapes only new videos (not already in the CSV), enriches and appends them, then runs categorize and focus on the full file.\n\n\n7. Publish\nThe catalog is built with Quarto: browse.qmd reads data/yoga_videos_enriched.csv and config/focus_map.json / config/exclude_keywords.json, then renders the interactive browse page (R + JSON + client-side JS).\n\n\n\nSo: scraping → enrichment (ETL) → categorization → focus update → Quarto render = automation from channel list to live catalog."
  },
  {
    "objectID": "projects/yoga-video-catalog/index.html#project-files",
    "href": "projects/yoga-video-catalog/index.html#project-files",
    "title": "Yoga Video Catalog",
    "section": "Project files",
    "text": "Project files\nThese files are included in this portfolio for reference. In the full project they live in the Yoga repo and are run from the project root.\n\nData and config\n\n\n\n\n\n\n\nPath\nPurpose\n\n\n\n\nyoga-video-catalog/data/yoga_videos_enriched.csv\nMain catalog data (video_id, title, channel, duration, focus, upload_date, url, etc.). Built by the pipeline.\n\n\nyoga-video-catalog/config/focus_map.json\nMaps canonical focus labels (e.g. “Core”, “Hips”) to keyword variations used to detect focus from title/description.\n\n\nyoga-video-catalog/config/exclude_keywords.json\nKeywords used to exclude videos (e.g. vlogs, announcements). Plus a focus-based filter so only videos with at least one focus tag appear.\n\n\n\n\n\nScripts (in yoga-video-catalog/scripts/)\n\n\n\n\n\n\n\nScript\nRole\n\n\n\n\nupdate_from_channels.py\nMain entry point. Reads channel config and existing CSV; for each channel scrapes only new videos, enriches them, appends to CSV; runs categorize and focus on the full file.\n\n\nscrape_channel.py\nPlaywright-based scraper: loads a channel’s Videos page, scrolls to load all, parses video links and durations. Can run standalone or via update_from_channels.py.\n\n\nenrich_videos.py\nyt-dlp enrichment: reads a CSV of video IDs, fetches metadata, derives focus from title/description using focus_map.json, writes enriched CSV (or appends to existing).\n\n\ncategorize_data.py\nAdds duration buckets, upload year, and view-count categories to the enriched CSV.\n\n\nupdate_focus.py\nRecomputes the focus column from title/description using the same keyword map (no re-fetch from YouTube)."
  },
  {
    "objectID": "projects/yoga-video-catalog/index.html#tech-stack",
    "href": "projects/yoga-video-catalog/index.html#tech-stack",
    "title": "Yoga Video Catalog",
    "section": "Tech stack",
    "text": "Tech stack\n\n\n\n\n\n\n\nLayer\nTools\n\n\n\n\nScraping\nPython, Playwright (Chromium)\n\n\nEnrichment\nyt-dlp, pandas\n\n\nConfig\nJSON (channel list, focus map, exclude keywords)\n\n\nCatalog UI\nQuarto, R (read CSV + inject JSON), vanilla JS (filters, queue, preview)"
  },
  {
    "objectID": "projects/yoga-video-catalog/index.html#takeaways",
    "href": "projects/yoga-video-catalog/index.html#takeaways",
    "title": "Yoga Video Catalog",
    "section": "Takeaways",
    "text": "Takeaways\n\nSingle command to add new channels or pull new videos from existing channels (update_from_channels.py), then re-render the catalog.\nNo YouTube API — scraping + yt-dlp keeps the pipeline simple and key-free.\nConfig-driven focus and exclusion make it easy to tune what appears in the catalog without changing code.\nEnd-to-end showcase: from raw channel pages to a polished, filterable catalog and queue, demonstrating web scraping, ETL, automation, and reproducible reporting with Quarto."
  },
  {
    "objectID": "projects/yoga-video-catalog/index.html#script-update-from-channels",
    "href": "projects/yoga-video-catalog/index.html#script-update-from-channels",
    "title": "Yoga Video Catalog",
    "section": "update_from_channels.py",
    "text": "update_from_channels.py\nWhat it does: Orchestrates the full update. Loads data/channel_urls.json and the existing enriched CSV; for each channel, calls the Playwright scraper to get only new videos (stops when it hits the first video already in the catalog). Writes new rows to a temp CSV, runs enrich_videos.py to fetch metadata via yt-dlp and append to the main CSV, then runs categorize_data.py and update_focus.py on the full file. Single entry point for “pull new videos and refresh the catalog.”\n\"\"\"\nUpdate the catalog with only NEW videos from each configured channel.\n\n- Reads data/channel_urls.json for channel name -&gt; videos page URL.\n- Reads data/yoga_videos_enriched.csv to get existing video_ids.\n- For each channel: scrapes the channel page, keeps only video_ids not already\n  in the spreadsheet, enriches those new videos, and appends them.\n- Then runs categorize_data.py and update_focus.py on the full file (categories + focus).\n\nRun without re-pulling or re-processing videos you already have.\n\nUsage:\n  python update_from_channels.py\n  python update_from_channels.py --headless\n  python update_from_channels.py --no-categorize   # skip categorize and body-parts steps\n\"\"\"\n\nimport argparse\nimport csv\nimport json\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nimport pandas as pd\n\nfrom scrape_channel import scrape_channel_new_only\n\nDATA_DIR = Path(__file__).parent / \"data\"\nCHANNEL_URLS_JSON = DATA_DIR / \"channel_urls.json\"\nENRICHED_CSV = DATA_DIR / \"yoga_videos_enriched.csv\"\nENRICHED_XLSX = DATA_DIR / \"yoga_videos_enriched.xlsx\"\n\n\ndef load_channel_config() -&gt; dict[str, str]:\n    \"\"\"Load channel display name -&gt; videos page URL.\"\"\"\n    path = CHANNEL_URLS_JSON.resolve()\n    if not path.exists():\n        print(f\"Config not found: {path}\")\n        print(\"Create it with JSON: {\\\"Channel Name\\\": \\\"https://www.youtube.com/@handle/videos\\\", ...}\")\n        return {}\n    with open(path, encoding=\"utf-8\") as f:\n        out = json.load(f)\n    if not isinstance(out, dict):\n        print(f\"Invalid config: {path} should be a JSON object.\")\n        return {}\n    return out\n\n\ndef load_existing_video_ids() -&gt; set[str]:\n    \"\"\"Return set of video_ids already in the enriched CSV.\"\"\"\n    if not ENRICHED_CSV.exists():\n        return set()\n    df = pd.read_csv(ENRICHED_CSV)\n    if \"video_id\" not in df.columns:\n        return set()\n    return set(df[\"video_id\"].astype(str).dropna().unique())\n\n\ndef main() -&gt; None:\n    parser = argparse.ArgumentParser(description=\"Update catalog with new videos only from configured channels\")\n    parser.add_argument(\"--headless\", action=\"store_true\", help=\"Run browser headless when scraping\")\n    parser.add_argument(\"--no-categorize\", action=\"store_true\", help=\"Skip categorize_data and update_focus\")\n    args = parser.parse_args()\n\n    print(\"Loading channel config...\")\n    print(f\"  Config path: {CHANNEL_URLS_JSON.resolve()}\")\n    channels = load_channel_config()\n    if not channels:\n        return\n    channel_list = list(channels.items())\n    print(f\"  Found {len(channel_list)} channel(s) to check.\")\n    for idx, (name, _) in enumerate(channel_list, 1):\n        print(f\"    {idx}. {name}\")\n\n    print(\"\\nLoading existing catalog...\")\n    existing_ids = load_existing_video_ids()\n    print(f\"  Existing videos in catalog: {len(existing_ids)}\")\n\n    script_dir = Path(__file__).resolve().parent\n    python_exe = sys.executable\n    any_new = False\n\n    for idx, (channel_name, channel_url) in enumerate(channel_list, 1):\n        print(f\"\\n--- Channel {idx}/{len(channel_list)}: {channel_name} ---\")\n        print(f\"  URL: {channel_url}\")\n        try:\n            new_videos = scrape_channel_new_only(\n                existing_ids,\n                channel_url,\n                channel_name=channel_name,\n                headless=args.headless,\n                log=print,\n            )\n        except Exception as e:\n            print(f\"  Scrape failed: {e}\")\n            import traceback\n            traceback.print_exc()\n            continue\n\n        print(f\"  New videos to add: {len(new_videos)}\")\n\n        if not new_videos:\n            print(\"  Nothing to add for this channel.\")\n            continue\n\n        any_new = True\n        temp_csv = DATA_DIR / f\"_new_{channel_name.replace(' ', '_')}.csv\"\n        fieldnames = [\"channel\", \"video_id\", \"title\", \"duration\", \"metadata_line\", \"url\"]\n        print(f\"  Writing {len(new_videos)} rows to temp file...\")\n        with open(temp_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n            writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction=\"ignore\")\n            writer.writeheader()\n            writer.writerows(new_videos)\n\n        print(f\"  Enriching {len(new_videos)} new videos (yt-dlp)...\")\n        subprocess.run(\n            [\n                python_exe,\n                str(script_dir / \"enrich_videos.py\"),\n                \"--input\", str(temp_csv),\n                \"--append-to\", str(ENRICHED_CSV),\n                \"--channel\", channel_name,\n            ],\n            check=True,\n            cwd=str(script_dir),\n        )\n        temp_csv.unlink(missing_ok=True)\n        existing_ids.update(v[\"video_id\"] for v in new_videos)\n        print(f\"  Appended to {ENRICHED_CSV}.\")\n\n    if not any_new:\n        print(\"\\nNo new videos to add. Catalog is up to date.\")\n        return\n\n    if args.no_categorize:\n        print(\"\\nSkipping categorize and body-parts (--no-categorize).\")\n        print(\"Done.\")\n        return\n\n    print(\"\\nUpdating categories on full file (categorize_data.py)...\")\n    subprocess.run([python_exe, str(script_dir / \"categorize_data.py\")], check=True, cwd=str(script_dir))\n    print(\"Updating focus (update_focus.py)...\")\n    subprocess.run([python_exe, str(script_dir / \"update_focus.py\")], check=True, cwd=str(script_dir))\n    print(\"\\nDone. Re-run quarto render yoga_catalog.qmd to refresh the catalog page.\")\n\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "projects/yoga-video-catalog/index.html#script-scrape-channel",
    "href": "projects/yoga-video-catalog/index.html#script-scrape-channel",
    "title": "Yoga Video Catalog",
    "section": "scrape_channel.py",
    "text": "scrape_channel.py\nWhat it does: Uses Playwright (Chromium) to open a YouTube channel’s Videos page, scrolls until all videos are loaded, and extracts video ID, title, duration, metadata line, and URL from each item. Exposes scrape_channel_to_list() for a full scrape and scrape_channel_new_only() which stops scrolling once it finds the first video already in a given set (used by the orchestrator to fetch only new videos). Can be run standalone with --url and --output for one-off scrapes.\n\"\"\"\nScrape a YouTube channel's videos page using Playwright.\nRun with: python scrape_channel.py [--url URL] [--output FILE] [--channel-name NAME]\nBrowser runs visible (headless=False) so you can watch it work.\n\"\"\"\n\nimport argparse\nimport csv\nimport re\nimport time\nfrom pathlib import Path\n\nfrom playwright.sync_api import sync_playwright\n\nDATA_DIR = Path(__file__).parent / \"data\"\nDEFAULT_URL = \"https://www.youtube.com/@yogawithadriene/videos\"\nDEFAULT_OUTPUT = DATA_DIR / \"yoga_videos.csv\"\n# How long to wait for the grid to load and between scrolls\nPAGE_LOAD_WAIT_MS = 4000\nSCROLL_PAUSE_SEC = 1.2\n# Stop after this many scrolls with no new videos\nMAX_EMPTY_SCROLLS = 5\n\n\ndef scroll_to_load_all(page) -&gt; None:\n    \"\"\"Scroll the page until no new videos appear.\"\"\"\n    last_count = 0\n    empty_scrolls = 0\n\n    while empty_scrolls &lt; MAX_EMPTY_SCROLLS:\n        # Scroll the main content (YouTube uses #content or the scrollable container)\n        page.evaluate(\"window.scrollBy(0, window.innerHeight)\")\n        time.sleep(SCROLL_PAUSE_SEC)\n\n        # Count current video links (links to /watch?v=)\n        count = page.locator('a[href^=\"/watch?v=\"]').count()\n        if count &gt; last_count:\n            last_count = count\n            empty_scrolls = 0\n        else:\n            empty_scrolls += 1\n\n    # One more scroll and wait to catch any final items\n    page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n    time.sleep(SCROLL_PAUSE_SEC)\n\n\ndef scroll_once(page) -&gt; None:\n    \"\"\"Scroll down one viewport height to load more content.\"\"\"\n    page.evaluate(\"window.scrollBy(0, window.innerHeight)\")\n    time.sleep(SCROLL_PAUSE_SEC)\n\n\ndef parse_duration(text: str) -&gt; str:\n    \"\"\"Return duration as-is (e.g. '12:34' or '1:23:45').\"\"\"\n    if not text or not text.strip():\n        return \"\"\n    return text.strip()\n\n\ndef parse_views(text: str) -&gt; str:\n    \"\"\"Return view count string as-is (e.g. '1.2M views').\"\"\"\n    if not text or not text.strip():\n        return \"\"\n    return text.strip()\n\n\ndef extract_videos(page) -&gt; list[dict]:\n    \"\"\"Extract video entries from the current page.\"\"\"\n    videos = []\n    # Video links in the grid (exclude sidebar/other links by using the grid context)\n    # YouTube shows each video in a rich-item-renderer with a link containing /watch?v=\n    links = page.locator('a[href^=\"/watch?v=\"]').all()\n\n    seen_ids = set()\n\n    for link in links:\n        try:\n            href = link.get_attribute(\"href\") or \"\"\n            if \"&\" in href:\n                video_id = href.split(\"&\")[0].replace(\"/watch?v=\", \"\")\n            else:\n                video_id = href.replace(\"/watch?v=\", \"\").strip()\n            if not video_id or video_id in seen_ids:\n                continue\n            seen_ids.add(video_id)\n\n            url = f\"https://www.youtube.com{href}\" if href.startswith(\"/\") else href\n\n            # Title: often in the link's title attribute or in an inner text element\n            title = link.get_attribute(\"title\") or \"\"\n            if not title:\n                title_el = link.locator(\"yt-formatted-string#video-title\").first\n                if title_el.count():\n                    title = title_el.text_content() or \"\"\n\n            # Duration: in a span that usually has time or duration (overlay on thumbnail)\n            duration = \"\"\n            container = link.locator(\"xpath=ancestor::ytd-rich-item-renderer\").first\n            if container.count():\n                duration_el = container.locator(\n                    \"span.ytd-thumbnail-overlay-time-status-renderer, \"\n                    \"#text.ytd-thumbnail-overlay-time-status-renderer, \"\n                    \"ytd-thumbnail-overlay-time-status-renderer span\"\n                ).first\n                if duration_el.count():\n                    duration = parse_duration(duration_el.text_content() or \"\")\n\n            # Views and date: in metadata line (e.g. \"1.2M views · 2 years ago\")\n            metadata = \"\"\n            if container.count():\n                meta_el = container.locator(\n                    \"ytd-video-meta-block #metadata-line span, \"\n                    \"span.ytd-video-meta-block\"\n                ).first\n                if meta_el.count():\n                    metadata = meta_el.text_content() or \"\"\n\n            videos.append({\n                \"video_id\": video_id,\n                \"title\": title.strip(),\n                \"duration\": duration,\n                \"metadata_line\": metadata.strip(),\n                \"url\": url,\n            })\n        except Exception as e:\n            print(f\"Skip one entry: {e}\")\n            continue\n\n    return videos\n\n\ndef slug_from_url(url: str) -&gt; str:\n    \"\"\"Return a short slug for the channel URL (e.g. @yogawithadriene -&gt; yogawithadriene).\"\"\"\n    if not url:\n        return \"channel\"\n    u = url.rstrip(\"/\").replace(\"https://www.youtube.com/\", \"\").replace(\"https://youtube.com/\", \"\")\n    return u.replace(\"@\", \"\").replace(\"/videos\", \"\").replace(\"/\", \"_\") or \"channel\"\n\n\ndef scrape_channel_to_list(\n    url: str,\n    channel_name: str = \"\",\n    headless: bool = False,\n) -&gt; list[dict]:\n    \"\"\"\n    Scrape a channel's videos page and return a list of video dicts.\n    Each dict has: video_id, title, duration, metadata_line, url; plus \"channel\" if channel_name is set.\n    \"\"\"\n    with sync_playwright() as p:\n        browser = p.chromium.launch(headless=headless)\n        context = browser.new_context(\n            viewport={\"width\": 1280, \"height\": 720},\n            user_agent=(\n                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n                \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n            ),\n        )\n        page = context.new_page()\n        try:\n            page.goto(url, wait_until=\"domcontentloaded\", timeout=60000)\n            page.wait_for_timeout(PAGE_LOAD_WAIT_MS)\n            scroll_to_load_all(page)\n            videos = extract_videos(page)\n            by_id = {}\n            for v in videos:\n                if v[\"video_id\"] not in by_id:\n                    by_id[v[\"video_id\"]] = v\n            videos = list(by_id.values())\n            if channel_name:\n                for v in videos:\n                    v[\"channel\"] = channel_name\n            return videos\n        finally:\n            browser.close()\n\n\ndef scrape_channel_new_only(\n    existing_ids: set[str],\n    url: str,\n    channel_name: str = \"\",\n    headless: bool = False,\n    log=None,\n) -&gt; list[dict]:\n    \"\"\"\n    Scrape a channel's videos page (newest first) and return only videos that are\n    not in existing_ids. Stops loading as soon as the first already-cataloged video\n    is found, since everything below it is older and already in the spreadsheet.\n    \"\"\"\n    if log is None:\n        log = print\n\n    with sync_playwright() as p:\n        browser = p.chromium.launch(headless=headless)\n        context = browser.new_context(\n            viewport={\"width\": 1280, \"height\": 720},\n            user_agent=(\n                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n                \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n            ),\n        )\n        page = context.new_page()\n        try:\n            log(\"  Opening channel page...\")\n            page.goto(url, wait_until=\"domcontentloaded\", timeout=60000)\n            page.wait_for_timeout(PAGE_LOAD_WAIT_MS)\n\n            prev_count = 0\n            empty_scrolls = 0\n\n            while empty_scrolls &lt; MAX_EMPTY_SCROLLS:\n                log(\"  Reading video list from page...\")\n                videos = extract_videos(page)\n                count = len(videos)\n                log(f\"  Loaded {count} videos so far (newest at top).\")\n\n                # Walk top-to-bottom (newest first); stop at first video already in catalog\n                for i, v in enumerate(videos):\n                    if v[\"video_id\"] in existing_ids:\n                        log(f\"  Found first cataloged video at position {i + 1} — stopping (all above are new).\")\n                        new_list = videos[:i]\n                        if channel_name:\n                            for u in new_list:\n                                u[\"channel\"] = channel_name\n                        return new_list\n\n                if count == prev_count and prev_count &gt; 0:\n                    empty_scrolls += 1\n                else:\n                    empty_scrolls = 0\n                prev_count = count\n\n                log(\"  No match yet — scrolling to load more...\")\n                scroll_once(page)\n\n            # Reached end of channel (no more videos load); all we have are new\n            log(\"  Reached end of channel list; treating all as new.\")\n            if channel_name:\n                for v in videos:\n                    v[\"channel\"] = channel_name\n            return videos\n        finally:\n            browser.close()\n\n\ndef main() -&gt; None:\n    parser = argparse.ArgumentParser(description=\"Scrape a YouTube channel's videos page\")\n    parser.add_argument(\"--url\", default=DEFAULT_URL, help=f\"Channel videos URL (default: {DEFAULT_URL})\")\n    parser.add_argument(\"--output\", type=Path, default=None, help=\"Output CSV path (default: data/yoga_videos_&lt;slug&gt;.csv for custom URL, else data/yoga_videos.csv)\")\n    parser.add_argument(\"--channel-name\", type=str, default=\"\", help=\"Channel display name (stored in 'channel' column if provided)\")\n    args = parser.parse_args()\n\n    url = args.url\n    channel_name = (args.channel_name or \"\").strip()\n    if args.output is not None:\n        output_csv = Path(args.output)\n    else:\n        if url == DEFAULT_URL:\n            output_csv = DEFAULT_OUTPUT\n        else:\n            slug = slug_from_url(url)\n            output_csv = DATA_DIR / f\"yoga_videos_{slug}.csv\"\n\n    output_csv.parent.mkdir(parents=True, exist_ok=True)\n\n    print(f\"Opening {url} ...\")\n    videos = scrape_channel_to_list(url, channel_name=channel_name, headless=False)\n    print(f\"Found {len(videos)} videos.\")\n\n    if not videos:\n        print(\"No videos extracted. Check the page structure or selectors.\")\n        return\n\n    fieldnames = [\"video_id\", \"title\", \"duration\", \"metadata_line\", \"url\"]\n    if channel_name:\n        fieldnames = [\"channel\"] + fieldnames\n\n    with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n        writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction=\"ignore\")\n        writer.writeheader()\n        writer.writerows(videos)\n\n    print(f\"Saved to {output_csv}\")\n\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "projects/yoga-video-catalog/index.html#script-enrich-videos",
    "href": "projects/yoga-video-catalog/index.html#script-enrich-videos",
    "title": "Yoga Video Catalog",
    "section": "enrich_videos.py",
    "text": "enrich_videos.py\nWhat it does: Reads a CSV of yoga videos (with a url column), uses yt-dlp (no API key) to fetch each video’s metadata (title, description, duration, upload_date, view_count, etc.), and derives a focus column by matching title and description against config/focus_map.json. Supports --append-to and --channel to add a new channel’s enriched rows to an existing CSV. Writes both CSV and optional Excel.\n\"\"\"\nEnrich a yoga videos CSV with per-video metadata using yt-dlp (no YouTube API).\nFetches: title, description, duration, upload_date, view_count, like_count, etc.\nAdds a focus column by parsing title + description with focus keywords.\n\nUsage:\n  python enrich_videos.py                           # process data/yoga_videos.csv -&gt; data/yoga_videos_enriched.csv\n  python enrich_videos.py --input data/foo.csv --output data/foo_enriched.csv --channel \"My Channel\"\n  python enrich_videos.py --input data/new.csv --append-to data/yoga_videos_enriched.csv --channel \"New Channel\"\n  python enrich_videos.py --limit 20 --delay 2\n\"\"\"\n\nimport argparse\nimport json\nimport re\nimport time\nfrom pathlib import Path\n\nimport pandas as pd\nimport yt_dlp\n\nDATA_DIR = Path(__file__).parent / \"data\"\nCONFIG_DIR = Path(__file__).parent / \"config\"\nDEFAULT_INPUT = DATA_DIR / \"yoga_videos.csv\"\nDEFAULT_OUTPUT = DATA_DIR / \"yoga_videos_enriched.csv\"\nDEFAULT_XLSX = DATA_DIR / \"yoga_videos_enriched.xlsx\"\n\nwith open(CONFIG_DIR / \"focus_map.json\", encoding=\"utf-8\") as f:\n    FOCUS_MAP = json.load(f)\n\nFOCUS_KEYWORDS = [(canonical, variations) for canonical, variations in FOCUS_MAP.items()]\n\n_SEPARATOR_RE = re.compile(\n    r\"^[\\s]*\"\n    r\"([-]{3,}|[—]{3,}|[-\\s]{5,}|[*]{3,}|[_]{3,}|[=]{3,}|[═]{3,}|[─]{3,})\"\n    r\"[\\s]*$\",\n    re.MULTILINE,\n)\n\n\ndef truncate_at_separator(desc: str) -&gt; str:\n    \"\"\"Return description text above the first visual separator line.\"\"\"\n    if not desc:\n        return \"\"\n    m = _SEPARATOR_RE.search(desc)\n    return desc[: m.start()].strip() if m else desc\n\n\ndef extract_focus(text: str) -&gt; list[str]:\n    \"\"\"Return list of canonical focus names found in text (lowercase).\"\"\"\n    if not text or not isinstance(text, str):\n        return []\n    text_lower = text.lower()\n    found = []\n    # Check in order (more specific first due to FOCUS_MAP ordering)\n    for canonical, variations in FOCUS_KEYWORDS:\n        if canonical in found:\n            continue\n        for variation in variations:\n            if variation in text_lower:\n                found.append(canonical)\n                break\n    return found\n\n\ndef fetch_video_metadata(url: str, ydl_opts: dict) -&gt; dict | None:\n    \"\"\"Return a flat dict of fields we want, or None on failure.\"\"\"\n    try:\n        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n            info = ydl.extract_info(url, download=False)\n            if not info:\n                return None\n            # Normalize to dict and take only the keys we need\n            if hasattr(ydl, \"sanitize_info\"):\n                info = ydl.sanitize_info(info)\n            elif hasattr(info, \"get\"):\n                info = dict(info)\n            else:\n                info = dict(info) if info else {}\n    except Exception as e:\n        print(f\"  Error: {e}\")\n        return None\n\n    title = info.get(\"title\") or \"\"\n    description = (info.get(\"description\") or \"\")[:10000]  # cap size\n    duration = info.get(\"duration\")\n    upload_date = info.get(\"upload_date\")  # YYYYMMDD\n    view_count = info.get(\"view_count\")\n    like_count = info.get(\"like_count\")\n    categories = info.get(\"categories\")\n    tags = info.get(\"tags\")\n\n    # Format duration as M:SS or H:MM:SS\n    duration_str = \"\"\n    if duration is not None and duration &gt;= 0:\n        h = int(duration) // 3600\n        m = (int(duration) % 3600) // 60\n        s = int(duration) % 60\n        if h &gt; 0:\n            duration_str = f\"{h}:{m:02d}:{s:02d}\"\n        else:\n            duration_str = f\"{m}:{s:02d}\"\n\n    # Format upload_date as YYYY-MM-DD\n    upload_date_str = \"\"\n    if upload_date and len(str(upload_date)) &gt;= 8:\n        u = str(upload_date)[:8]\n        upload_date_str = f\"{u[:4]}-{u[4:6]}-{u[6:8]}\"\n\n    focus_list = extract_focus(title + \" \" + truncate_at_separator(description))\n    focus_str = \" | \".join(focus_list) if focus_list else \"\"\n\n    return {\n        \"title\": title,\n        \"description\": description,\n        \"duration_seconds\": duration if duration is not None else \"\",\n        \"duration\": duration_str or \"\",\n        \"upload_date\": upload_date_str,\n        \"view_count\": view_count if view_count is not None else \"\",\n        \"like_count\": like_count if like_count is not None else \"\",\n        \"categories\": \" | \".join(categories) if isinstance(categories, list) else (categories or \"\"),\n        \"tags\": \" | \".join(tags) if isinstance(tags, list) else (tags or \"\"),\n        \"focus\": focus_str,\n    }\n\n\ndef main() -&gt; None:\n    parser = argparse.ArgumentParser(description=\"Enrich yoga videos CSV with yt-dlp metadata\")\n    parser.add_argument(\"--input\", type=Path, default=DEFAULT_INPUT, help=\"Input CSV path\")\n    parser.add_argument(\"--output\", type=Path, default=None, help=\"Output CSV path (default: data/yoga_videos_enriched.csv or same as --append-to)\")\n    parser.add_argument(\"--append-to\", type=Path, default=None, help=\"Append enriched rows to this CSV (and update same-name .xlsx). Use with --input and --channel to add a new channel.\")\n    parser.add_argument(\"--channel\", type=str, default=\"\", help=\"Channel display name (added as 'channel' column; required when using --append-to for new channel)\")\n    parser.add_argument(\"--limit\", type=int, default=None, help=\"Process only first N rows (for testing)\")\n    parser.add_argument(\"--delay\", type=float, default=1.5, help=\"Seconds between requests (default 1.5)\")\n    parser.add_argument(\"--no-excel\", action=\"store_true\", help=\"Skip writing .xlsx file\")\n    args = parser.parse_args()\n\n    input_csv = Path(args.input)\n    append_to = Path(args.append_to) if args.append_to else None\n    channel_name = (args.channel or \"\").strip()\n\n    if append_to:\n        output_csv = append_to\n        output_xlsx = append_to.with_suffix(\".xlsx\")\n    else:\n        output_csv = Path(args.output) if args.output else DEFAULT_OUTPUT\n        output_xlsx = output_csv.with_suffix(\".xlsx\") if not args.no_excel else None\n\n    if not input_csv.exists():\n        print(f\"Input not found: {input_csv}\")\n        return\n\n    df = pd.read_csv(input_csv)\n    if \"url\" not in df.columns:\n        print(\"CSV must have a 'url' column\")\n        return\n\n    if append_to and not channel_name:\n        print(\"When using --append-to you must set --channel so rows are labeled.\")\n        return\n\n    urls = df[\"url\"].astype(str).tolist()\n    if args.limit:\n        urls = urls[: args.limit]\n        df = df.iloc[: len(urls)].copy()\n\n    if channel_name and \"channel\" not in df.columns:\n        df.insert(0, \"channel\", channel_name)\n\n    ydl_opts = {\n        \"quiet\": True,\n        \"no_warnings\": True,\n        \"extract_flat\": False,\n        \"skip_download\": True,\n    }\n\n    enriched_rows = []\n    for i, url in enumerate(urls):\n        if not url or not url.startswith(\"http\"):\n            enriched_rows.append({k: \"\" for k in (\"title\", \"description\", \"duration_seconds\", \"duration\", \"upload_date\", \"view_count\", \"like_count\", \"categories\", \"tags\", \"focus\")})\n            continue\n        print(f\"[{i + 1}/{len(urls)}] {url[:60]}...\")\n        row = fetch_video_metadata(url, ydl_opts)\n        if row:\n            enriched_rows.append(row)\n        else:\n            enriched_rows.append({k: \"\" for k in (\"title\", \"description\", \"duration_seconds\", \"duration\", \"upload_date\", \"view_count\", \"like_count\", \"categories\", \"tags\", \"focus\")})\n        time.sleep(args.delay)\n\n    enrich_df = pd.DataFrame(enriched_rows)\n    out = df.copy()\n    for col in enrich_df.columns:\n        out[col] = enrich_df[col].values\n    original_cols = [c for c in df.columns]\n    new_cols = [c for c in enrich_df.columns if c not in df.columns]\n    out = out[original_cols + new_cols]\n\n    if append_to and append_to.exists():\n        existing = pd.read_csv(append_to)\n        if \"channel\" not in existing.columns:\n            existing.insert(0, \"channel\", \"Yoga With Adriene\")\n        out = pd.concat([existing, out], ignore_index=True)\n\n    DATA_DIR.mkdir(parents=True, exist_ok=True)\n    out.to_csv(output_csv, index=False, encoding=\"utf-8\")\n    print(f\"Saved {output_csv}\")\n\n    if not args.no_excel and output_xlsx:\n        out.to_excel(output_xlsx, index=False, engine=\"openpyxl\")\n        print(f\"Saved {output_xlsx}\")\n\n    print(\"Done.\")\n\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "projects/yoga-video-catalog/index.html#script-categorize-data",
    "href": "projects/yoga-video-catalog/index.html#script-categorize-data",
    "title": "Yoga Video Catalog",
    "section": "categorize_data.py",
    "text": "categorize_data.py\nWhat it does: Reads data/yoga_videos_enriched.csv and adds derived columns: duration_mins and duration_bucket (rounded to nearest 5 min, capped at 120), duration_category for display; upload_year from upload_date; views_category (Under 500k, 500k–1M, 1M–2.5M, etc.) and views_category_order for sorting. Overwrites the CSV and optional Excel. Run after enrichment so the catalog filters have consistent buckets.\n\"\"\"\nAdd category columns to yoga_videos_enriched.csv:\n  - duration_bucket: round to nearest 5 min (0-5 min rounds to 5, then normal rounding)\n  - upload_year: year only\n  - views_category: one of Under 500k, 500k–1M, 1M–2.5M, 2.5M–10M, 10M+\n  - views_category_order: 1–5 for sorting\n\nRun after enrich_videos.py. Overwrites the CSV with added columns (keeps all existing columns).\n\"\"\"\n\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nDATA_DIR = Path(__file__).parent / \"data\"\nINPUT_CSV = DATA_DIR / \"yoga_videos_enriched.csv\"\nOUTPUT_CSV = DATA_DIR / \"yoga_videos_enriched.csv\"\nOUTPUT_XLSX = DATA_DIR / \"yoga_videos_enriched.xlsx\"\n\nVIEWS_BREAKS = [0, 500_000, 1_000_000, 2_500_000, 10_000_000]\nVIEWS_LABELS = [\"Under 500k\", \"500k–1M\", \"1M–2.5M\", \"2.5M–10M\", \"10M+\"]\n\n\ndef duration_to_mins(s):\n    \"\"\"Parse 'M:SS' or 'H:MM:SS'. For 3 parts: if value as H:MM:SS would be &gt; 1:59:59 (120 min), assume MM:SS:xx (e.g. 2:00:00 = 2 min); else H:MM:SS.\"\"\"\n    if pd.isna(s) or str(s).strip() == \"\":\n        return None\n    parts = str(s).strip().split(\":\")\n    parts = [float(x) for x in parts if x.replace(\".\", \"\").isdigit()]\n    if not parts:\n        return None\n    if len(parts) == 2:\n        return parts[0] + parts[1] / 60\n    if len(parts) == 3:\n        as_hms = parts[0] * 60 + parts[1] + parts[2] / 60\n        if as_hms &gt;= 120:\n            return parts[0] + parts[1] / 60  # MM:SS:xx (e.g. 25:42:00 or 2:00:00)\n        return as_hms  # H:MM:SS (1:59:59 or less)\n    return None\n\n\ndef main():\n    if not INPUT_CSV.exists():\n        print(f\"Not found: {INPUT_CSV}\")\n        return\n\n    df = pd.read_csv(INPUT_CSV)\n\n    # Duration: round to nearest 5 min (but 0-5 min rounds to 5, not 0)\n    if \"duration_seconds\" in df.columns:\n        sec = pd.to_numeric(df[\"duration_seconds\"], errors=\"coerce\")\n        df[\"duration_mins\"] = sec / 60\n    else:\n        df[\"duration_mins\"] = df[\"duration\"].map(duration_to_mins)\n    df[\"duration_mins\"] = df[\"duration_mins\"].fillna(0)\n    # Round to nearest 5, but if result is 0 and original &gt; 0, set to 5; cap at 120 so bad/very long durations don't show as 999\n    df[\"duration_bucket\"] = (np.round(df[\"duration_mins\"] / 5) * 5).clip(upper=120).astype(int)\n    df.loc[(df[\"duration_bucket\"] == 0) & (df[\"duration_mins\"] &gt; 0), \"duration_bucket\"] = 5\n    df[\"duration_category\"] = np.where(\n        df[\"duration_bucket\"] == 0, \"—\",\n        np.where(df[\"duration_bucket\"] == 120,\n                 np.where(df[\"duration_mins\"] &gt; 120, \"120+ min\", \"120 min\"),\n                 df[\"duration_bucket\"].astype(str) + \" min\")\n    )\n\n    # Upload date -&gt; year\n    df[\"upload_year\"] = pd.to_datetime(df[\"upload_date\"], errors=\"coerce\").dt.year\n    df[\"upload_year\"] = df[\"upload_year\"].fillna(0).astype(int)\n    df[\"upload_year_display\"] = df[\"upload_year\"].replace(0, \"\").astype(str).replace(\"\", \"—\")\n\n    # Views: 5 categories\n    views = pd.to_numeric(df[\"view_count\"], errors=\"coerce\").fillna(-1)\n    order = np.searchsorted(VIEWS_BREAKS[1:] + [np.inf], views, side=\"left\") + 1\n    order[views &lt; 0] = 0\n    order = np.clip(order, 0, 5)\n    df[\"views_category_order\"] = order.astype(int)\n    df[\"views_category\"] = [VIEWS_LABELS[i - 1] if 1 &lt;= i &lt;= 5 else \"—\" for i in df[\"views_category_order\"]]\n    df.loc[df[\"view_count\"].isna(), \"views_category\"] = \"—\"\n\n    df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n    print(f\"Updated {OUTPUT_CSV} with duration_category, upload_year, views_category.\")\n    try:\n        df.to_excel(OUTPUT_XLSX, index=False, engine=\"openpyxl\")\n        print(f\"Updated {OUTPUT_XLSX}.\")\n    except Exception as e:\n        print(f\"Note: Could not update Excel: {e}\")\n\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "projects/yoga-video-catalog/index.html#script-update-focus",
    "href": "projects/yoga-video-catalog/index.html#script-update-focus",
    "title": "Yoga Video Catalog",
    "section": "update_focus.py",
    "text": "update_focus.py\nWhat it does: Recomputes the focus column in the enriched CSV using the same keyword logic as enrich_videos.py (reads config/focus_map.json, matches title + description). Does not call YouTube or yt-dlp — use this after changing the focus map or exclude keywords so existing rows get updated focus tags without re-fetching.\n\"\"\"\nUpdate focus column in yoga_videos_enriched.csv using the focus keyword list\n(same as enrich_videos.py).\n\nRun this to update existing enriched data without re-fetching from YouTube.\nThe CSV must have a column named **focus** (see INSTRUCTIONS.md if yours says body_part_focus).\n\"\"\"\n\nimport json\nimport re\nfrom pathlib import Path\n\nimport pandas as pd\n\nDATA_DIR = Path(__file__).parent / \"data\"\nCONFIG_DIR = Path(__file__).parent / \"config\"\nCSV_FILE = DATA_DIR / \"yoga_videos_enriched.csv\"\n\nwith open(CONFIG_DIR / \"focus_map.json\", encoding=\"utf-8\") as f:\n    FOCUS_MAP = json.load(f)\n\nFOCUS_KEYWORDS = [(canonical, variations) for canonical, variations in FOCUS_MAP.items()]\n\n_SEPARATOR_RE = re.compile(\n    r\"^[\\s]*\"\n    r\"([-]{3,}|[—]{3,}|[-\\s]{5,}|[*]{3,}|[_]{3,}|[=]{3,}|[═]{3,}|[─]{3,})\"\n    r\"[\\s]*$\",\n    re.MULTILINE,\n)\n\n\ndef truncate_at_separator(desc: str) -&gt; str:\n    \"\"\"Return description text above the first visual separator line.\"\"\"\n    if not desc:\n        return \"\"\n    m = _SEPARATOR_RE.search(desc)\n    return desc[: m.start()].strip() if m else desc\n\n\ndef extract_focus(text: str) -&gt; list[str]:\n    \"\"\"Return list of canonical focus names found in text (lowercase).\"\"\"\n    if not text or not isinstance(text, str):\n        return []\n    text_lower = text.lower()\n    found = []\n    for canonical, variations in FOCUS_KEYWORDS:\n        if canonical in found:\n            continue\n        for variation in variations:\n            if variation in text_lower:\n                found.append(canonical)\n                break\n    return found\n\n\ndef main():\n    if not CSV_FILE.exists():\n        print(f\"Not found: {CSV_FILE}\")\n        return\n\n    df = pd.read_csv(CSV_FILE, na_values=[\"\", \"NA\"])\n\n    # CSV must use \"focus\" column (rename body_part_focus in the CSV header if needed — see INSTRUCTIONS.md)\n    if \"focus\" not in df.columns and \"body_part_focus\" in df.columns:\n        print(\"The CSV has 'body_part_focus' but this script expects 'focus'.\")\n        print(\"Rename the column: open data/yoga_videos_enriched.csv, change the header in the first row from body_part_focus to focus, save, then run this again.\")\n        return\n    if \"body_part_focus\" in df.columns:\n        df = df.drop(columns=[\"body_part_focus\"])\n\n    titles = df[\"title\"].fillna(\"\")\n    descs = df[\"description\"].fillna(\"\").apply(truncate_at_separator)\n    title_desc = (titles + \" \" + descs).str[:10000]\n\n    # Extract focus from keywords\n    focus_lists = title_desc.apply(extract_focus)\n    df[\"focus\"] = focus_lists.apply(lambda x: \" | \".join(x) if x else \"\")\n\n    df.to_csv(CSV_FILE, index=False, encoding=\"utf-8\")\n    print(f\"Updated focus in {CSV_FILE}\")\n    print(f\"Videos with focus tags: {(df['focus'] != '').sum()} / {len(df)}\")\n\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/vignettes/README_vignette_maps.html",
    "href": "projects/Leaflet_Mapping_Tool/vignettes/README_vignette_maps.html",
    "title": "Vignette maps",
    "section": "",
    "text": "The Vignettes page embeds pre-generated Leaflet maps so the webpage loads quickly and does not hit OpenStreetMap/Overpass rate limits when visitors view it.\n\n\n\nFrom the project root, ensure you have built the income GeoPackage:\nsource(\"R/build_ses_index.R\")\nRun the map-generation script (also from project root):\nsource(\"vignettes/build_vignette_maps.R\")\nThis will:\n\nCreate each example map and write it to output/maps/\nCopy each file into vignettes/maps/ with the filenames expected by the vignette iframes\n\n\nRun the script when you want to refresh the embedded maps (e.g. after changing buffer or bbox). If you see Overpass rate-limit messages, wait a few minutes between runs or run the script in stages.\n\n\n\n\nHamilton_bivariate.html\nHamilton_expanded_bivariate.html (from run with custom_distance_km = 5)\nHamilton_1km_green_layers_bivariate.html (from run with green_buffer_m = 1000, show_green_layers = TRUE)\nCustom_bbox_bivariate.html (e.g. Drumheller badlands from build script)\nHalifax_bivariate.html\nCharlottetown_bivariate.html\nBrandon_bivariate.html\nYellowknife_bivariate.html\nInternational_placeholder.html (included in repo; replace with a real map when you have non-Canadian data)\n\n\n\n\nAfter quarto render, the site is in _site/. The iframes reference maps/..., which resolves to _site/vignettes/maps/ when viewing the vignette page. Ensure vignettes/maps/*.html (and their _files/ folders if not self-contained) are present before rendering, or Quarto may not copy them. If maps are missing, add vignettes/maps to your project resources or copy the folder into _site/vignettes/ after rendering."
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/vignettes/README_vignette_maps.html#one-time-setup",
    "href": "projects/Leaflet_Mapping_Tool/vignettes/README_vignette_maps.html#one-time-setup",
    "title": "Vignette maps",
    "section": "",
    "text": "From the project root, ensure you have built the income GeoPackage:\nsource(\"R/build_ses_index.R\")\nRun the map-generation script (also from project root):\nsource(\"vignettes/build_vignette_maps.R\")\nThis will:\n\nCreate each example map and write it to output/maps/\nCopy each file into vignettes/maps/ with the filenames expected by the vignette iframes\n\n\nRun the script when you want to refresh the embedded maps (e.g. after changing buffer or bbox). If you see Overpass rate-limit messages, wait a few minutes between runs or run the script in stages."
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/vignettes/README_vignette_maps.html#expected-files-in-vignettesmaps",
    "href": "projects/Leaflet_Mapping_Tool/vignettes/README_vignette_maps.html#expected-files-in-vignettesmaps",
    "title": "Vignette maps",
    "section": "",
    "text": "Hamilton_bivariate.html\nHamilton_expanded_bivariate.html (from run with custom_distance_km = 5)\nHamilton_1km_green_layers_bivariate.html (from run with green_buffer_m = 1000, show_green_layers = TRUE)\nCustom_bbox_bivariate.html (e.g. Drumheller badlands from build script)\nHalifax_bivariate.html\nCharlottetown_bivariate.html\nBrandon_bivariate.html\nYellowknife_bivariate.html\nInternational_placeholder.html (included in repo; replace with a real map when you have non-Canadian data)"
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/vignettes/README_vignette_maps.html#publishing-the-site",
    "href": "projects/Leaflet_Mapping_Tool/vignettes/README_vignette_maps.html#publishing-the-site",
    "title": "Vignette maps",
    "section": "",
    "text": "After quarto render, the site is in _site/. The iframes reference maps/..., which resolves to _site/vignettes/maps/ when viewing the vignette page. Ensure vignettes/maps/*.html (and their _files/ folders if not self-contained) are present before rendering, or Quarto may not copy them. If maps are missing, add vignettes/maps to your project resources or copy the folder into _site/vignettes/ after rendering."
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/index.html",
    "href": "projects/Leaflet_Mapping_Tool/index.html",
    "title": "Leaflet Mapping Tool",
    "section": "",
    "text": "Final product\n\n\nProject Overview\n\n\nScripts & Code"
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/index.html#problem",
    "href": "projects/Leaflet_Mapping_Tool/index.html#problem",
    "title": "Leaflet Mapping Tool",
    "section": "Problem",
    "text": "Problem\nA Statistics Canada field team needed a way to identify safe areas to stay while conducting months‑long data collection in cities across Canada. For each of the 16 survey cities, they required:\n\n\nmaps detailed enough to show building‑level satellite imagery\n50+ static maps per city to cover different neighbourhoods and safety considerations\na solution that could be updated every two years and used by staff without GIS expertise The existing approach (i.e., manually producing dozens of static maps) was time‑consuming, difficult to maintain, and not reproducible by the client’s team."
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/index.html#approach",
    "href": "projects/Leaflet_Mapping_Tool/index.html#approach",
    "title": "Leaflet Mapping Tool",
    "section": "Approach",
    "text": "Approach\nTo address the scale and complexity of the request, I designed an interactive mapping tool using OpenStreetMap data and R. This approach allowed me to:\n\n\nreplace hundreds of static maps with a single, dynamic map per city\nincorporate zooming, panning, basemap switching, and layer toggling\nallow users to click features to reveal additional information\nbuild a workflow that could be run by non‑GIS staff with basic R skills The tool automated data retrieval, processing, and map generation, making it adaptable to any Canadian city—and scalable beyond Canada if needed."
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/index.html#impact",
    "href": "projects/Leaflet_Mapping_Tool/index.html#impact",
    "title": "Leaflet Mapping Tool",
    "section": "Impact",
    "text": "Impact\n\nReduced 50+ maps per city to one interactive map, improving usability and reducing cognitive load for field teams.\nEliminated dependency on GIS specialists, enabling analysts with basic R skills to generate maps independently.\nCreated a future‑proof workflow that can be reused every two years for recurring surveys and adapted for other Statistics Canada programs.\nImproved consistency and reproducibility, ensuring that all cities follow the same mapping logic and data standards."
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/index.html#note",
    "href": "projects/Leaflet_Mapping_Tool/index.html#note",
    "title": "Leaflet Mapping Tool",
    "section": "Note",
    "text": "Note\nBecause safety and crime data are not publicly available, the public version of this tool demonstrates its capabilities using median household income and access to greenspace. The underlying workflow remains the same, and I’ve also extended the tool to support bivariate mapping to showcase its flexibility. If you’re interested in adapting this tool for your own project, feel free to reach out - I’m happy to help!"
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/index.html#script-create-bivariate-map",
    "href": "projects/Leaflet_Mapping_Tool/index.html#script-create-bivariate-map",
    "title": "Leaflet Mapping Tool",
    "section": "create_bivariate_map.R",
    "text": "create_bivariate_map.R\nWhat it does: Main entry point for the bivariate map. Builds an Income × Green area per 1,000 people Leaflet map for a given city or custom bbox. Sources map_config, process_bbox, and process_green_da; clips DAs to the study area, fetches OSM greenspace (with optional walkable buffer), classifies income and greenspace into Low/Med/High (mean ± 1 SD), and renders a 3×3 bivariate choropleth with popups and legend. Supports custom_bbox, custom_distance_km, green_buffer_m, and show_green_layers.\n\n\nCode\n#\n# Bivariate map: Income × Green area per 1000 people (greenspace within walkable buffer). 3 classes each (±1 SD), 9 colours.\n# Income = median total income of household (2020 $); classified Low/Med/High by study-area mean ± 1 SD.\n# Requires: income GeoPackage (e.g. data/ses_index.gpkg from build_ses_index.R or data/income.gpkg) with 'population' and 'income_raw'; process_bbox is called automatically if needed.\n# Usage: source(\"R/create_bivariate_map.R\"); create_bivariate_map(city = \"Halifax, NS\", income_gpkg = \"data/ses_index.gpkg\")\n#\n# Popup: Income (median household $), Income Category, Greenspace (area, population, per 1000, category), Description.\n#\n\nif (dir.exists(\"R\")) {\n  source(\"R/map_config.R\")\n  source(\"R/process_bbox.R\")\n  source(\"R/process_green_da.R\")\n} else {\n  source(\"map_config.R\")\n  source(\"process_bbox.R\")\n  source(\"process_green_da.R\")\n}\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(leaflet)\nlibrary(htmltools)\nlibrary(htmlwidgets)\n\n# 3x3 bivariate palette: Income × Green area (user-specified)\nBIVAR_COLOURS &lt;- c(\n  \"#F3F3F3\", \"#C2F1CE\", \"#8BE2AF\",   # Income Low  × Green Low, Med, High\n  \"#EAC5DD\", \"#9EC6D3\", \"#7FC6B1\",   # Income Med  × Green Low, Med, High\n  \"#E6A3D0\", \"#BC9FCE\", \"#7B8EAF\"     # Income High × Green Low, Med, High\n)\nnames(BIVAR_COLOURS) &lt;- c(\n  \"Low-Low\", \"Low-Med\", \"Low-High\",\n  \"Med-Low\", \"Med-Med\", \"Med-High\",\n  \"High-Low\", \"High-Med\", \"High-High\"\n)\n\n# Bivariate category descriptions (row = Income, col = Greenspace)\nBIVAR_DESCRIPTIONS &lt;- c(\n  \"Low-Low\"  = \"Low income with limited greenspace access.\",\n  \"Low-Med\"  = \"Low income with moderate greenspace access.\",\n  \"Low-High\" = \"Low income with high greenspace access.\",\n  \"Med-Low\"  = \"Moderate income with low greenspace access.\",\n  \"Med-Med\"  = \"Moderate income with moderate greenspace access.\",\n  \"Med-High\" = \"Moderate income with high greenspace access.\",\n  \"High-Low\" = \"High income with low greenspace access.\",\n  \"High-Med\" = \"High income with moderate greenspace access.\",\n  \"High-High\"= \"High income with high greenspace access.\"\n)\n\n# Convert hex colour to rgba with given alpha (for legend transparency)\nhex_to_rgba &lt;- function(hex, alpha = 0.6) {\n  r &lt;- strtoi(substr(hex, 2, 3), 16L)\n  g &lt;- strtoi(substr(hex, 4, 5), 16L)\n  b &lt;- strtoi(substr(hex, 6, 7), 16L)\n  sprintf(\"rgba(%d,%d,%d,%.2f)\", r, g, b, alpha)\n}\n\n# Build HTML for 3x3 legend: y-axis = Greenspace (dependent), x-axis = Income (independent).\n# Grid: rows = Green High/Med/Low, cols = Income Low/Med/High. Colour at (green_row, income_col) = same (income, green) combo.\nbivariate_legend_html &lt;- function(fill_alpha = 1) {\n  # BIVAR_COLOURS indexed as (income_row, green_col); income_row 1=Low,2=Med,3=High; green_col 1=Low,2=Med,3=High.\n  # Legend rows = Green (1=High, 2=Med, 3=Low), cols = Income (1=Low, 2=Med, 3=High). So colour idx = (income_col - 1)*3 + (4 - green_row).\n  cell &lt;- function(green_row, income_col) {\n    i &lt;- (income_col - 1L) * 3L + (4L - green_row)\n    col_rgba &lt;- hex_to_rgba(BIVAR_COLOURS[i], fill_alpha)\n    sprintf('&lt;div style=\"width:26px;height:26px;background:%s;border:1px solid #555;\"&gt;&lt;/div&gt;', col_rgba)\n  }\n  y_label_rotated &lt;- '&lt;div style=\"writing-mode:vertical-rl;transform:rotate(-180deg);font-size:9px;height:78px;line-height:1.35;text-align:center;\"&gt;Green area per&lt;br&gt;1,000 people&lt;/div&gt;'\n  rows &lt;- c(\n    paste0('&lt;tr&gt;&lt;td rowspan=\"3\" style=\"vertical-align:middle;width:1px;padding:0 4px 0 0;\"&gt;', y_label_rotated, '&lt;/td&gt;',\n           '&lt;td style=\"text-align:right;padding-right:4px;font-size:9px;vertical-align:middle;\"&gt;High&lt;/td&gt;',\n           '&lt;td&gt;', cell(1, 1), '&lt;/td&gt;&lt;td&gt;', cell(1, 2), '&lt;/td&gt;&lt;td&gt;', cell(1, 3), '&lt;/td&gt;&lt;/tr&gt;'),\n    paste0('&lt;tr&gt;&lt;td style=\"text-align:right;padding-right:4px;font-size:9px;vertical-align:middle;\"&gt;Med&lt;/td&gt;',\n           '&lt;td&gt;', cell(2, 1), '&lt;/td&gt;&lt;td&gt;', cell(2, 2), '&lt;/td&gt;&lt;td&gt;', cell(2, 3), '&lt;/td&gt;&lt;/tr&gt;'),\n    paste0('&lt;tr&gt;&lt;td style=\"text-align:right;padding-right:4px;font-size:9px;vertical-align:middle;\"&gt;Low&lt;/td&gt;',\n           '&lt;td&gt;', cell(3, 1), '&lt;/td&gt;&lt;td&gt;', cell(3, 2), '&lt;/td&gt;&lt;td&gt;', cell(3, 3), '&lt;/td&gt;&lt;/tr&gt;'),\n    '&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td style=\"text-align:center;font-size:9px;padding-top:2px;\"&gt;Low&lt;/td&gt;&lt;td style=\"text-align:center;font-size:9px;padding-top:2px;\"&gt;Med&lt;/td&gt;&lt;td style=\"text-align:center;font-size:9px;padding-top:2px;\"&gt;High&lt;/td&gt;&lt;/tr&gt;',\n    '&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td colspan=\"3\" style=\"text-align:center;font-size:9px;padding-top:4px;line-height:1.35;\"&gt;Median Household&lt;br&gt;Income&lt;/td&gt;&lt;/tr&gt;'\n  )\n  paste0(\n    '&lt;div class=\"leaflet-bivariate-legend\" style=\"z-index:9999;position:relative;background:white;padding:10px;border-radius:4px;box-shadow:0 1px 5px rgba(0,0,0,0.3);max-width:260px;\"&gt;',\n    '&lt;div style=\"font-weight:bold;font-size:11px;margin-bottom:6px;\"&gt;Exploring how median household income influences greenspace access by dissemination area, 2020&lt;/div&gt;',\n    '&lt;div style=\"font-size:9px;margin-bottom:8px;line-height:1.35;\"&gt;&lt;b&gt;Interpretation:&lt;/b&gt;&lt;br&gt;',\n    '&lt;b&gt;Blues:&lt;/b&gt; Equal greenspace access to median household income.&lt;br&gt;',\n    '&lt;b&gt;Greens:&lt;/b&gt; Equitable greenspace access for DAs with low median household income.&lt;br&gt;',\n    '&lt;b&gt;Pinks:&lt;/b&gt; Low greenspace access for DAs with high median household income.&lt;/div&gt;',\n    '&lt;table style=\"border-collapse:collapse;\"&gt;&lt;tbody&gt;', paste(rows, collapse = \"\"), '&lt;/tbody&gt;&lt;/table&gt;',\n    '&lt;/div&gt;'\n  )\n}\n\n# For testing: write the bivariate legend to a standalone HTML file and optionally open it.\n# Usage: source(\"R/create_bivariate_map.R\"); preview_legend()\n# Edit bivariate_legend_html() or BIVAR_COLOURS, then run preview_legend() again to refresh.\npreview_legend &lt;- function(open = TRUE, file = \"output/maps/legend_preview.html\") {\n  dir.create(dirname(file), recursive = TRUE, showWarnings = FALSE)\n  html &lt;- paste0(\n    \"&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset='UTF-8'&gt;&lt;/head&gt;&lt;body style='margin:20px;'&gt;\",\n    bivariate_legend_html(),\n    \"&lt;/body&gt;&lt;/html&gt;\"\n  )\n  writeLines(html, file)\n  message(\"Legend written to \", normalizePath(file, mustWork = FALSE))\n  if (open) {\n    if (requireNamespace(\"rstudioapi\", quietly = TRUE) && rstudioapi::isAvailable()) {\n      rstudioapi::viewer(file)\n    } else {\n      utils::browseURL(file)\n    }\n  }\n  invisible(file)\n}\n\ncreate_bivariate_map &lt;- function(city, income_gpkg, amalgamation_cities_list = NULL, city_pr = NULL,\n                                 custom_bbox = NULL, custom_distance_km = 0, green_buffer_m = 500,\n                                 show_green_layers = FALSE) {\n  # green_buffer_m: for each DA, greenspace is summed inside the DA plus within this distance (m) of the DA. Zone = DA + buffer (default 500 m).\n\n  message(\"Step 1 of 5: Processing bounding box and clipping income layer...\")\n  expanded_bbox &lt;- process_bbox(\n    study_area = city,\n    income_gpkg = income_gpkg,\n    amalgamation_cities_list = amalgamation_cities_list,\n    city_pr = city_pr,\n    custom_bbox = custom_bbox,\n    custom_distance_km = custom_distance_km\n  )\n\n  modified_text &lt;- str_replace_all(str_extract(city, \"^[^,]+\"), \" \", \"_\")\n  income_dir &lt;- file.path(dirname(income_gpkg), \"clipped_income\")\n  city_income_path &lt;- file.path(income_dir, paste0(modified_text, \".gpkg\"))\n  if (!file.exists(city_income_path)) stop(\"Clipped income layer not found: \", city_income_path, \" Run process_bbox first (e.g. by running this function once with the same city).\")\n\n  city_shp &lt;- st_read(city_income_path, quiet = TRUE)\n  if (nrow(city_shp) == 0) stop(\"No polygons in study area.\")\n  if (!\"population\" %in% names(city_shp)) stop(\"Clipped layer missing 'population'. Ensure income layer includes CHARACTERISTIC_ID 1 for total population.\")\n\n  message(\"Step 2 of 5: Computing green area per DA (OSM, buffer \", green_buffer_m, \" m)...\")\n  city_shp &lt;- process_green_da(city_shp, expanded_bbox, buffer_m = green_buffer_m, city = city)\n  n_na_green &lt;- sum(is.na(city_shp$green_area_per_1000))\n  if (n_na_green == nrow(city_shp) && nrow(city_shp) &gt; 0) {\n    warning(\n      \"Green area per 1,000 people is NA for all DAs. Likely cause: 'population' is missing or zero in the census. \",\n      \"Ensure your income layer includes CHARACTERISTIC_ID 1 (Total population), then process_bbox and this map.\",\n      call. = FALSE\n    )\n  }\n\n  # Prefer income_raw (median household $); fallback to ses_idx or index if present (e.g. from build_ses_index.R)\n  idx_col &lt;- intersect(c(\"ses_idx\", \"income_raw\", \"index\"), names(city_shp))[1]\n  if (is.na(idx_col)) idx_col &lt;- names(city_shp)[sapply(city_shp, is.numeric)][1]\n  city_shp$plot_income &lt;- city_shp[[idx_col]]\n  if (\"income_raw\" %in% names(city_shp)) {\n    city_shp$plot_income[is.na(city_shp$income_raw)] &lt;- NA_real_\n  }\n  green_var &lt;- \"green_area_per_1000\"\n\n  # 3 classes each: ±1 SD (Low &lt; mean - sd, High &gt; mean + sd, else Medium)\n  mean_income   &lt;- mean(city_shp$plot_income, na.rm = TRUE)\n  sd_income     &lt;- sd(city_shp$plot_income, na.rm = TRUE)\n  mean_green &lt;- mean(city_shp[[green_var]], na.rm = TRUE)\n  sd_green   &lt;- sd(city_shp[[green_var]], na.rm = TRUE)\n  if (is.na(sd_income) || sd_income == 0) sd_income &lt;- 1e-9\n  if (is.na(sd_green) || sd_green == 0) sd_green &lt;- 1e-9\n\n  city_shp &lt;- city_shp %&gt;%\n    mutate(\n      class_income   = case_when(\n        is.na(plot_income) ~ NA_character_,\n        plot_income &lt; mean_income - sd_income ~ \"Low\",\n        plot_income &gt; mean_income + sd_income ~ \"High\",\n        TRUE ~ \"Med\"\n      ),\n      class_green = case_when(\n        .data[[green_var]] &lt; mean_green - sd_green ~ \"Low\",\n        .data[[green_var]] &gt; mean_green + sd_green ~ \"High\",\n        TRUE ~ \"Med\"\n      )\n    ) %&gt;%\n    mutate(\n      class_green = replace_na(class_green, \"Low\"),\n      bivar = if_else(is.na(class_income) | is.na(class_green), NA_character_,\n                      paste(class_income, class_green, sep = \"-\"))\n    )\n  city_shp$bivar_colour &lt;- BIVAR_COLOURS[city_shp$bivar]\n  city_shp$bivar_colour[is.na(city_shp$bivar_colour)] &lt;- \"#CBCBCB\"\n  city_shp$bivar_opacity &lt;- if_else(is.na(city_shp$bivar), 0, 0.6)\n  # Normal stroke for non-extreme DAs; inner stroke only for extreme (corner) DAs\n  BIVAR_EXTREMES &lt;- c(\"Low-Low\", \"High-High\", \"High-Low\", \"Low-High\")\n  city_shp$bivar_edge_color &lt;- if_else(\n    is.na(city_shp$bivar), \"transparent\",\n    if_else(city_shp$bivar %in% BIVAR_EXTREMES, \"#FFBB00\", \"white\")\n  )\n  city_shp$bivar_edge_weight &lt;- if_else(is.na(city_shp$bivar), 0L, if_else(city_shp$bivar %in% BIVAR_EXTREMES, 1.5, 0.5))\n  city_shp$polygon_stroke_color &lt;- if_else(\n    is.na(city_shp$bivar), \"transparent\",\n    if_else(city_shp$bivar %in% BIVAR_EXTREMES, \"transparent\", \"white\")\n  )\n  city_shp$polygon_stroke_weight &lt;- if_else(is.na(city_shp$bivar), 0L, if_else(city_shp$bivar %in% BIVAR_EXTREMES, 0L, 0.5))\n\n  bbox_polygon &lt;- st_as_sfc(st_bbox(c(\n    xmin = expanded_bbox[\"x\", \"min\"], ymin = expanded_bbox[\"y\", \"min\"],\n    xmax = expanded_bbox[\"x\", \"max\"], ymax = expanded_bbox[\"y\", \"max\"]\n  ), crs = st_crs(4326)))\n\n  message(\"Step 3 of 5: Building Income × Greenspace map...\")\n  green_na &lt;- is.na(city_shp[[green_var]])\n\n  fmt_km2 &lt;- function(x) format(round(x / 1e6, 1), big.mark = \",\", nsmall = 1, trim = TRUE)\n  fmt_pop &lt;- function(x) if_else(is.na(x) | !is.finite(x), \"\\u2014\", format(as.integer(x), big.mark = \",\", trim = TRUE))\n\n  green_km2_str &lt;- fmt_km2(city_shp$green_area_m2)\n  green_per_1000_km2 &lt;- city_shp[[green_var]] / 1e6\n  green_per_1000_str &lt;- if_else(\n    green_na,\n    if_else(!is.finite(city_shp$population) | city_shp$population &lt;= 0,\n            \"No data (missing or zero population)\", \"No data\"),\n    paste0(format(round(green_per_1000_km2, 1), big.mark = \",\", nsmall = 1, trim = TRUE), \" km\\u00b2\")\n  )\n\n  fmt_income &lt;- function(x) if_else(is.na(x) | !is.finite(x), \"\\u2014\", paste0(\"$\", format(round(x, 0), big.mark = \",\", trim = TRUE)))\n  has_income_raw &lt;- \"income_raw\" %in% names(city_shp)\n\n  bivar_desc &lt;- BIVAR_DESCRIPTIONS[city_shp$bivar]\n  bivar_desc[is.na(bivar_desc)] &lt;- \"\"\n\n  city_shp &lt;- city_shp %&gt;%\n    mutate(\n      popup_text = paste0(\n        \"&lt;b style='font-size:13px;'&gt;Income&lt;/b&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Median household income:&lt;/b&gt; \", if (has_income_raw) fmt_income(income_raw) else fmt_income(plot_income), \"&lt;br&gt;\",\n        \"&lt;b&gt;Income Category:&lt;/b&gt; \", if_else(is.na(class_income), \"\\u2014\", class_income),\n        \"&lt;br&gt;&lt;br&gt;\",\n        \"&lt;b style='font-size:13px;'&gt;Greenspace&lt;/b&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Green area (in buffer):&lt;/b&gt; \", green_km2_str, \" km\\u00b2&lt;br&gt;\",\n        \"&lt;b&gt;Population:&lt;/b&gt; \", fmt_pop(population), \"&lt;br&gt;\",\n        \"&lt;b&gt;Green area per 1,000 people:&lt;/b&gt; \", green_per_1000_str, \"&lt;br&gt;\",\n        \"&lt;b&gt;Green area Category:&lt;/b&gt; \", if_else(is.na(class_green), \"\\u2014\", class_green),\n        \"&lt;br&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Description:&lt;/b&gt; \", bivar_desc\n      )\n    )\n\n  # Inner-boundary stroke so outline is drawn inward and always visible (no overlap at shared edges)\n  utm_zone &lt;- floor((expanded_bbox[\"x\", \"min\"] + expanded_bbox[\"x\", \"max\"]) / 2 + 180) %/% 6 + 1\n  crs_planar &lt;- if (expanded_bbox[\"y\", \"min\"] + expanded_bbox[\"y\", \"max\"] &gt; 0) {\n    st_crs(paste0(\"+proj=utm +zone=\", utm_zone, \" +datum=WGS84 +units=m +no_defs\"))\n  } else {\n    st_crs(paste0(\"+proj=utm +zone=\", utm_zone, \" +south +datum=WGS84 +units=m +no_defs\"))\n  }\n  inner_offset_m &lt;- 3\n  city_utm_geom &lt;- st_transform(st_geometry(city_shp), crs_planar)\n  inner_buf &lt;- st_buffer(city_utm_geom, -inner_offset_m)\n  inner_boundary &lt;- st_boundary(inner_buf)\n  empty_idx &lt;- st_is_empty(inner_boundary)\n  if (any(empty_idx)) {\n    orig_boundary &lt;- st_boundary(city_utm_geom)\n    inner_boundary[empty_idx] &lt;- orig_boundary[empty_idx]\n  }\n  inner_boundary_wgs84 &lt;- st_transform(inner_boundary, 4326)\n  city_shp_inner &lt;- st_sf(\n    bivar_edge_color = city_shp$bivar_edge_color,\n    bivar_edge_weight = city_shp$bivar_edge_weight,\n    geometry = inner_boundary_wgs84,\n    crs = st_crs(4326)\n  )\n  extreme_idx &lt;- city_shp$bivar %in% BIVAR_EXTREMES\n  city_shp_inner_extreme &lt;- city_shp_inner[extreme_idx, ]\n\n  map_leaflet &lt;- leaflet(options = leafletOptions(zoomControl = TRUE)) %&gt;%\n    fitBounds(\n      lng1 = expanded_bbox[\"x\", \"min\"], lat1 = expanded_bbox[\"y\", \"min\"],\n      lng2 = expanded_bbox[\"x\", \"max\"], lat2 = expanded_bbox[\"y\", \"max\"]\n    ) %&gt;%\n    addProviderTiles(providers$Esri.WorldGrayCanvas, group = \"Light\") %&gt;%\n    addTiles(group = \"Street\") %&gt;%\n    addProviderTiles(providers$Esri.WorldImagery, group = \"Satellite\") %&gt;%\n    addPolygons(\n      data = bbox_polygon,\n      color = \"black\", weight = 2, fill = FALSE, label = \"Study area\",\n      group = \"Study area\"\n    ) %&gt;%\n    addPolygons(\n      data = city_shp,\n      fillColor = ~bivar_colour,\n      color = ~polygon_stroke_color,\n      weight = ~polygon_stroke_weight,\n      fillOpacity = ~bivar_opacity,\n      popup = ~popup_text,\n      group = \"Income × Green\"\n    )\n  if (nrow(city_shp_inner_extreme) &gt; 0) {\n    map_leaflet &lt;- map_leaflet %&gt;%\n      addPolylines(\n        data = city_shp_inner_extreme,\n        color = ~bivar_edge_color,\n        weight = ~bivar_edge_weight,\n        opacity = 1,\n        group = \"Income × Green\"\n      )\n  }\n\n  # Merged greenspace (one lightweight unioned layer for display)\n  message(\"Fetching merged greenspace layer (OSM) for display...\")\n  merged_green &lt;- fetch_greenspace_merged(expanded_bbox, city = city)\n  if (is.null(merged_green) || nrow(merged_green) == 0) {\n    message(\"  No merged greenspace layer returned (OSM query may have failed or been rate limited). Map will show DAs only.\")\n  }\n  if (!is.null(merged_green) && nrow(merged_green) &gt; 0) {\n    map_leaflet &lt;- map_leaflet %&gt;%\n      addPolygons(\n        data = merged_green,\n        fillColor = \"#228B22\", fillOpacity = 0.4, color = \"#006400\", weight = 1,\n        group = \"Greenspace (merged)\"\n      )\n  }\n\n  # Per-type greenspace layers (optional; disabled by default because thousands of polygons\n  # make the HTML huge and can prevent the layer control from rendering)\n  green_group_names &lt;- character(0)\n  if (show_green_layers) {\n    message(\"Fetching greenspace layers by OSM type (show_green_layers = TRUE)...\")\n    green_layers &lt;- fetch_greenspace_layers(expanded_bbox)\n    for (i in seq_along(green_layers)) {\n      grp &lt;- green_layers[[i]]$group\n      sf_layer &lt;- green_layers[[i]]$sf\n      map_leaflet &lt;- map_leaflet %&gt;%\n        addPolygons(\n          data = sf_layer,\n          fillColor = \"#228B22\", fillOpacity = 0.35, color = \"#006400\", weight = 1,\n          group = grp\n        )\n      green_group_names &lt;- c(green_group_names, grp)\n    }\n  }\n\n  overlay_grps &lt;- c(\"Income × Green\", \"Study area\", if (!is.null(merged_green) && nrow(merged_green) &gt; 0) \"Greenspace (merged)\", green_group_names)\n\n  opacity_slider_html &lt;- paste0(\n    '&lt;div class=\"leaflet-opacity-control\" style=\"background:white;padding:8px 10px;border-radius:4px;box-shadow:0 1px 5px rgba(0,0,0,0.3);min-width:140px;\"&gt;',\n    '&lt;label style=\"display:block;font-size:11px;font-weight:bold;margin-bottom:4px;\"&gt;Overlay opacity&lt;/label&gt;',\n    '&lt;input type=\"range\" min=\"0\" max=\"100\" value=\"60\" data-overlay-opacity-slider style=\"width:100%;cursor:pointer;\"&gt;',\n    '&lt;/div&gt;'\n  )\n\n  map_leaflet &lt;- map_leaflet %&gt;%\n    addLayersControl(\n      position = \"topright\",\n      baseGroups = c(\"Light\", \"Street\", \"Satellite\"),\n      overlayGroups = overlay_grps,\n      options = layersControlOptions(collapsed = FALSE)\n    ) %&gt;%\n    addControl(\n      html = HTML(bivariate_legend_html()),\n      position = \"bottomleft\"\n    ) %&gt;%\n    addControl(\n      html = HTML(opacity_slider_html),\n      position = \"bottomright\"\n    ) %&gt;%\n    htmlwidgets::onRender(\n      \"function(el, x) {\n        var w = HTMLWidgets.find('#' + el.id);\n        if (!w || typeof w.getMap !== 'function') return;\n        var map = w.getMap();\n        if (!map) return;\n        var lm = map.layerManager;\n        if (!lm || typeof lm.getLayerGroup !== 'function') return;\n        var overlayGroups = ['Study area', 'Income × Green', 'Greenspace (merged)'];\n        function applyOpacityToGroup(groupName, v) {\n          var group = lm.getLayerGroup(groupName, false);\n          if (group && group.eachLayer) {\n            group.eachLayer(function(l) {\n              if (l.setStyle) l.setStyle({ fillOpacity: v, opacity: v });\n            });\n          }\n        }\n        function setOverlayOpacity(pct) {\n          var v = Math.max(0, Math.min(1, pct / 100));\n          overlayGroups.forEach(function(name) { applyOpacityToGroup(name, v); });\n        }\n        function init() {\n          function hideGreenspace() {\n            if (typeof map.hideGroup === 'function') {\n              map.hideGroup('Greenspace (merged)');\n            } else {\n              var g = lm.getLayerGroup('Greenspace (merged)', false);\n              if (g && map.hasLayer && map.hasLayer(g)) map.removeLayer(g);\n            }\n          }\n          hideGreenspace();\n          setTimeout(hideGreenspace, 100);\n          var slider = el.querySelector('input[data-overlay-opacity-slider]');\n          if (slider) {\n            setOverlayOpacity(parseInt(slider.value, 10));\n            slider.addEventListener('input', function() {\n              setOverlayOpacity(parseInt(slider.value, 10));\n            });\n            var control = slider.closest('.leaflet-opacity-control') || slider.parentElement;\n            if (control) {\n              ['mousedown', 'touchstart', 'click', 'dblclick'].forEach(function(ev) {\n                control.addEventListener(ev, function(e) { e.stopPropagation(); }, true);\n              });\n            }\n          }\n        }\n        if (map.whenReady) {\n          map.whenReady(init);\n        } else {\n          setTimeout(init, 150);\n        }\n      }\"\n    )\n\n  message(\"Step 4 of 5: Saving map...\")\n  out_dir &lt;- \"output/maps\"\n  dir.create(out_dir, recursive = TRUE, showWarnings = FALSE)\n  out_file &lt;- file.path(normalizePath(out_dir, mustWork = FALSE), paste0(modified_text, \"_bivariate.html\"))\n  saveWidget(map_leaflet, file = out_file, selfcontained = FALSE)\n  message(\"Step 5 of 5: Done. Map saved to \", out_file)\n  message(\"  Open the HTML in a browser; keep the _files/ folder next to it.\")\n  invisible(map_leaflet)\n}"
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/index.html#script-mapping-functions",
    "href": "projects/Leaflet_Mapping_Tool/index.html#script-mapping-functions",
    "title": "Leaflet Mapping Tool",
    "section": "mapping_functions.R",
    "text": "mapping_functions.R\nWhat it does: Helper functions for the Leaflet map: create_custom_icon, add_markers_conditionally, add_polylines_conditionally, and add_polygons_conditionally to add OSM features (parks, playgrounds, nature, etc.) as toggleable layers with popups. Used when show_green_layers = TRUE.\n\n\nCode\nlibrary(leaflet)\nlibrary(htmltools)\nlibrary(glue)\nlibrary(dplyr)\n\ncreate_custom_icon &lt;- function(icon_url, icon_width = 40, icon_height = 40) {\n  makeIcon(\n    iconUrl = icon_url,\n    iconWidth = icon_width,\n    iconHeight = icon_height,\n    iconAnchorX = icon_width / 2,\n    iconAnchorY = 0\n  )\n}\n\nadd_markers_conditionally &lt;- function(map, data, group_name, icon_url, icon_width = 40, icon_height = 40) {\n  if (!is.null(data) && nrow(data) &gt; 0) {\n    data &lt;- data %&gt;%\n      mutate(\n        popup_info = glue(\n          \"&lt;b&gt;Type:&lt;/b&gt; {if ('amenity' %in% colnames(data)) amenity else if ('leisure' %in% colnames(data)) leisure else 'Green space'}&lt;br&gt;\",\n          \"&lt;b&gt;Name:&lt;/b&gt; {if ('name' %in% colnames(data)) ifelse(!is.na(name), name, 'No name available') else 'No name available'}\"\n        )\n      )\n    custom_icon &lt;- create_custom_icon(icon_url, icon_width = icon_width, icon_height = icon_height)\n    map &lt;- map %&gt;%\n      addMarkers(data = data, icon = custom_icon, popup = ~popup_info, group = group_name)\n  }\n  return(map)\n}\n\nadd_polylines_conditionally &lt;- function(map, data, group_name, color) {\n  if (!is.null(data) && nrow(data) &gt; 0) {\n    data &lt;- data %&gt;%\n      mutate(\n        popup_info = ifelse(\n          \"name\" %in% colnames(data),\n          glue(\"&lt;b&gt;Type:&lt;/b&gt; {highway}&lt;br&gt;&lt;b&gt;Name:&lt;/b&gt; {name}\"),\n          glue(\"&lt;b&gt;Type:&lt;/b&gt; {highway}&lt;br&gt;&lt;b&gt;Name:&lt;/b&gt; No name available\")\n        )\n      )\n    map &lt;- map %&gt;%\n      addPolylines(data = data, color = color, weight = 3, opacity = 0.8, popup = ~popup_info, group = group_name)\n  }\n  return(map)\n}\n\nadd_polygons_conditionally &lt;- function(map, data, group_name, fill_color) {\n  if (!is.null(data) && nrow(data) &gt; 0) {\n    data &lt;- data %&gt;%\n      mutate(\n        popup_info = ifelse(\n          \"name\" %in% colnames(data),\n          glue(\"&lt;b&gt;Type:&lt;/b&gt; {amenity}&lt;br&gt;&lt;b&gt;Name:&lt;/b&gt; {name}\"),\n          glue(\"&lt;b&gt;Type:&lt;/b&gt; {amenity}&lt;br&gt;&lt;b&gt;Name:&lt;/b&gt; No name available\")\n        )\n      )\n    map &lt;- map %&gt;%\n      addPolygons(data = data, fillColor = fill_color, color = \"black\", weight = 1, fillOpacity = 0.6, popup = ~popup_info, group = group_name)\n  }\n  return(map)\n}"
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/index.html#script-map-config",
    "href": "projects/Leaflet_Mapping_Tool/index.html#script-map-config",
    "title": "Leaflet Mapping Tool",
    "section": "map_config.R",
    "text": "map_config.R\nWhat it does: Central configuration list for SES (income) categories and green-space settings. Defines ses (labels, colours, emoji, SD multipliers for 5 income bands) and green (thresholds for Limited/Good/Great, icon URLs for parks/playgrounds/nature). Used by build_ses_index.R and process_green.R; create_bivariate_map.R uses its own bivariate palette.\n\n\nCode\n#\n# Configuration for SES/income index and green space.\n#\n# Used by:\n#   build_ses_index.R  - ses$labels, ses$sd_multipliers (income status categories in output)\n#   process_green.R    - green$thresholds (Limited/Good/Great counts), green$icon_* (markers)\n#   create_bivariate_map.R - uses its own bivariate palette; does not use this config\n#\n# Edit this file to change index categories, colours, icons, or green-space thresholds.\n#\n\nmap_config &lt;- list(\n\n  # --- SES (socioeconomic status) ---\n  # Order: [1] = Lowest ... [5] = Highest. Same order for colours, labels, and emoji (for popups).\n  # Colours: Lowest=red, Low=yellow, Average=green, High=blue, Highest=purple (matches pop-up emojis).\n  ses = list(\n    labels = c(\"Lowest\", \"Low\", \"Average\", \"High\", \"Highest\"),\n    colours = c(\"#D72638\", \"#FFCC00\", \"#3DBB61\", \"#0078D7\", \"#6F008C\"),\n    # Emoji shown beside category in popup (National Comparison): 🔴 Lowest, 🟡 Low, 🟢 Average, 🔵 High, 🟣 Highest\n    emoji = c(\"🔴\", \"🟡\", \"🟢\", \"🔵\", \"🟣\"),\n    # SD multipliers for break boundaries: mean + sd_multipliers * sd (4 values =&gt; 5 categories)\n    sd_multipliers = c(-2, -1, 1, 2)\n  ),\n\n  # --- Green space ---\n  green = list(\n    # Count thresholds for green status (parks + playgrounds + nature in study area)\n    # Above great =&gt; \"Great\", above good =&gt; \"Good\", above limited =&gt; \"Limited\", else \"None\"\n    thresholds = c(great = 20L, good = 5L, limited = 1L),\n    # Marker icon URLs\n    icon_url = list(\n      parks           = \"https://cdn-icons-png.flaticon.com/512/284/284648.png\",\n      playgrounds     = \"https://cdn-icons-png.flaticon.com/512/619/619032.png\",\n      nature_reserves = \"https://cdn-icons-png.flaticon.com/512/3073/3073484.png\"\n    ),\n    icon_width  = 40L,\n    icon_height = 40L\n  )\n)"
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/index.html#script-process-green-da",
    "href": "projects/Leaflet_Mapping_Tool/index.html#script-process-green-da",
    "title": "Leaflet Mapping Tool",
    "section": "process_green_da.R",
    "text": "process_green_da.R\nWhat it does: Computes green area per dissemination area (and per 1,000 people). For each DA, sums OSM greenspace (parks, gardens, meadows, etc.) inside the DA plus within a walkable buffer (e.g. 500 m). Fetches OSM for the study bbox, merges polygon types, intersects with buffered DAs, and returns DAs with green_area_m2 and green_area_per_1000. Handles Overpass rate limiting and uses cache_utils for optional caching.\n\n\nCode\n#\n# Green area per DA (and per 1000 people). Uses multiple OSM greenspace types and optional\n# walkable buffer around each DA.\n# Input: da_sf (clipped DAs with 'population'), bbox (matrix x/y min/max, WGS84).\n# Optional: buffer_m = walkable distance in metres (e.g. 500 m).\n#\n# For each DA, greenspace area is summed over: (1) the DA itself, plus (2) the area\n# within buffer_m of the DA boundary. So the zone is \"inside the DA + within X m of\n# the DA\" (st_buffer(DA, buffer_m) gives exactly that). This reflects access to\n# greenspace both inside and just outside the DA (reduces MAUP; helps edge DAs).\n# OSM greenspace is fetched for bbox only; use a larger bbox (e.g. custom_distance_km)\n# if you want full counts where the buffer extends beyond the study area.\n#\n# Returns: da_sf with green_area_m2 and green_area_per_1000 added.\n#\n\nlibrary(sf)\nlibrary(dplyr)\nlibrary(osmdata)\nif (dir.exists(\"R\")) source(\"R/cache_utils.R\") else source(\"cache_utils.R\")\n\n# Check Overpass/OSM error message for rate limiting or known API errors; warn and return TRUE if so\nwarn_if_overpass_error &lt;- function(e) {\n  msg &lt;- conditionMessage(e)\n  if (!is.character(msg) || length(msg) == 0) return(FALSE)\n  rate_limit &lt;- grepl(\"429|503|rate limit|rate-limited|too many requests|service unavailable|timeout|timed out|overpass\", msg, ignore.case = TRUE)\n  if (rate_limit) {\n    warning(\n      \"OSM/Overpass API may be rate limiting or overloaded. \",\n      \"Error: \", msg, \" \",\n      \"Wait a few minutes and re-run, or use set_overpass_url() in osmdata to try another server.\",\n      call. = FALSE\n    )\n    return(TRUE)\n  }\n  FALSE\n}\n\n# OSM key/value pairs for public greenspace (parks, gardens, forests, meadows, etc.)\n# OSM key/value pairs merged into the greenspace layer (polygons + multipolygons)\nOSM_GREEN_FEATURES &lt;- list(\n  list(key = \"leisure\", value = \"dog_park\"),\n  list(key = \"leisure\", value = \"garden\"),\n  list(key = \"leisure\", value = \"park\"),\n  list(key = \"leisure\", value = \"pitch\"),\n  list(key = \"landcover\", value = \"grass\"),\n  list(key = \"landuse\", value = \"meadow\"),\n  list(key = \"landuse\", value = \"recreation_ground\"),\n  list(key = \"landuse\", value = \"village_green\"),\n  list(key = \"landuse\", value = \"grass\"),\n  list(key = \"landuse\", value = \"greenery\"),\n  list(key = \"landuse\", value = \"allotments\"),\n  list(key = \"natural\", value = \"heath\"),\n  list(key = \"natural\", value = \"scrub\"),\n  list(key = \"natural\", value = \"wetland\"),\n  list(key = \"natural\", value = \"grassland\"),\n  list(key = \"natural\", value = \"shrubbery\"),\n  list(key = \"natural\", value = \"tundra\"),\n  list(key = \"natural\", value = \"wood\")\n)\n\nprocess_green_da &lt;- function(da_sf, bbox, buffer_m = 2000, city = NULL) {\n  if (is.null(da_sf) || nrow(da_sf) == 0 || !\"population\" %in% names(da_sf)) {\n    da_sf$green_area_m2 &lt;- NA_real_\n    da_sf$green_area_per_1000 &lt;- NA_real_\n    return(da_sf)\n  }\n  # Coerce population to numeric (GPKG/read_sf can sometimes return character)\n  da_sf$population &lt;- suppressWarnings(as.numeric(da_sf$population))\n  bbox_vec &lt;- c(bbox[\"x\", \"min\"], bbox[\"y\", \"min\"], bbox[\"x\", \"max\"], bbox[\"y\", \"max\"])\n\n  # Planar CRS for area (m²) and buffer\n  utm_zone &lt;- floor((bbox[\"x\", \"min\"] + bbox[\"x\", \"max\"]) / 2 + 180) %/% 6 + 1\n  if (bbox[\"y\", \"min\"] + bbox[\"y\", \"max\"] &gt; 0) {\n    crs_planar &lt;- st_crs(paste0(\"+proj=utm +zone=\", utm_zone, \" +datum=WGS84 +units=m +no_defs\"))\n  } else {\n    crs_planar &lt;- st_crs(paste0(\"+proj=utm +zone=\", utm_zone, \" +south +datum=WGS84 +units=m +no_defs\"))\n  }\n\n  da_sf$._da_row &lt;- seq_len(nrow(da_sf))\n  da_utm &lt;- st_transform(da_sf %&gt;% select(._da_row), crs_planar)\n\n  # --- Greenspace: try cache (by bbox UID), else fetch OSM and merge ---\n  greens_union &lt;- NULL\n  greens_cached &lt;- get_cached_greenspace(bbox)\n  if (!is.null(greens_cached) && nrow(greens_cached) &gt; 0) {\n    greens_utm &lt;- st_transform(greens_cached, crs_planar)\n    if (!all(st_is_valid(greens_utm))) greens_utm &lt;- st_make_valid(greens_utm)\n    greens_union &lt;- st_union(st_geometry(greens_utm))\n    if (length(greens_union) &gt; 1) greens_union &lt;- st_union(greens_union)\n    message(\"Using cached greenspace for this bbox. Green area uses buffer \", buffer_m, \" m.\")\n  }\n\n  if (is.null(greens_union)) {\n    # Fetch all greenspace polygon types and combine (polygons + multipolygons so e.g. natural=wood is included)\n    collect_polygons &lt;- function(key, value) {\n      tryCatch({\n        od &lt;- opq(bbox_vec) %&gt;%\n          add_osm_feature(key = key, value = value) %&gt;%\n          osmdata_sf()\n        polys &lt;- od$osm_polygons\n        multi &lt;- od$osm_multipolygons\n        if (!is.null(multi) && nrow(multi) &gt; 0) {\n          multi &lt;- st_cast(multi %&gt;% select(geometry), \"POLYGON\", warn = FALSE)\n          if (is.null(polys) || nrow(polys) == 0) polys &lt;- multi\n          else polys &lt;- rbind(polys %&gt;% select(geometry), multi)\n        }\n        if (is.null(polys) || nrow(polys) == 0) NULL else polys\n      }, error = function(e) {\n        warn_if_overpass_error(e)\n        NULL\n      })\n    }\n    poly_list &lt;- lapply(OSM_GREEN_FEATURES, function(f) collect_polygons(f$key, f$value))\n    poly_list &lt;- poly_list[!sapply(poly_list, is.null)]\n    poly_list &lt;- poly_list[sapply(poly_list, nrow) &gt; 0]\n\n    if (length(poly_list) == 0) {\n      warning(\n        \"No OSM greenspace polygons returned for this bbox. \",\n        \"If you ran this repeatedly, Overpass may be rate limiting; wait a few minutes and try again.\",\n        call. = FALSE\n      )\n      da_sf$green_area_m2 &lt;- 0\n      da_sf$green_area_per_1000 &lt;- 0\n      da_sf$._da_row &lt;- NULL\n      return(da_sf)\n    }\n\n    greens_sf &lt;- do.call(rbind, lapply(poly_list, function(x) x %&gt;% select(geometry)))\n    if (!all(st_is_valid(greens_sf))) greens_sf &lt;- st_make_valid(greens_sf)\n    greens_utm &lt;- st_transform(greens_sf, crs_planar)\n    if (!all(st_is_valid(greens_utm))) greens_utm &lt;- st_make_valid(greens_utm)\n    greens_geom &lt;- st_geometry(greens_utm)\n\n    n_poly &lt;- nrow(greens_utm)\n    greens_union &lt;- tryCatch(\n      st_union(greens_geom),\n      error = function(e) {\n        message(\"st_union failed (\", conditionMessage(e), \"); trying buffer(0) to fix topology...\")\n        fixed &lt;- tryCatch(st_buffer(greens_utm, 0), error = function(e2) greens_utm)\n        if (!all(st_is_valid(fixed))) fixed &lt;- st_make_valid(fixed)\n        tryCatch(\n          st_union(st_geometry(fixed)),\n          error = function(e2) {\n            message(\"buffer(0) union failed; trying chunked union...\")\n            chunk &lt;- 200L\n            idx &lt;- seq_len(nrow(fixed))\n            chunks &lt;- split(idx, ceiling(idx / chunk))\n            parts &lt;- lapply(chunks, function(i) st_union(st_geometry(fixed[i, ])))\n            Reduce(st_union, parts)\n          }\n        )\n      }\n    )\n    if (length(greens_union) &gt; 1) greens_union &lt;- st_union(greens_union)\n    message(\"Merged \", n_poly, \" greenspace polygons into one layer (overlaps counted once). Green area uses buffer \", buffer_m, \" m.\")\n    # Cache merged greenspace (WGS84)\n    greens_sf_planar &lt;- st_sf(geometry = st_sfc(greens_union, crs = crs_planar))\n    set_cached_greenspace(bbox, st_transform(greens_sf_planar, 4326), city = city)\n  }\n\n  # --- Buffered DAs: try cache (by bbox UID + buffer_m), else compute ---\n  cached_buf &lt;- get_cached_buffers(bbox, buffer_m)\n  use_cached_buf &lt;- FALSE\n  if (!is.null(cached_buf) && nrow(cached_buf) &gt; 0 &&\n      all(da_sf$._da_row %in% cached_buf$da_row) && nrow(cached_buf) == nrow(da_sf)) {\n    geoms &lt;- st_geometry(st_transform(cached_buf, crs_planar))\n    da_utm &lt;- st_sf(\n      ._da_row = da_sf$._da_row,\n      geometry = geoms[match(da_sf$._da_row, cached_buf$da_row)],\n      crs = crs_planar\n    )\n    use_cached_buf &lt;- TRUE\n    message(\"Using cached buffered DAs for this bbox and buffer \", buffer_m, \" m.\")\n  }\n\n  # Zone for summing greenspace = DA plus buffer (so greenspace inside DA + within buffer_m counts)\n  if (!use_cached_buf && buffer_m &gt; 0) {\n    da_utm &lt;- st_buffer(da_utm, buffer_m)\n    set_cached_buffers(bbox, buffer_m, da_utm, city = city)\n  }\n\n  int &lt;- tryCatch({\n    st_intersection(da_utm, greens_union)\n  }, error = function(e) NULL)\n\n  if (is.null(int) || nrow(int) == 0) {\n    da_sf$green_area_m2 &lt;- 0\n    da_sf$green_area_per_1000 &lt;- 0\n    da_sf$._da_row &lt;- NULL\n    return(da_sf)\n  }\n\n  int$area_m2 &lt;- as.numeric(st_area(int))\n  agg &lt;- int %&gt;%\n    st_drop_geometry() %&gt;%\n    group_by(._da_row) %&gt;%\n    summarise(green_area_m2 = sum(area_m2, na.rm = TRUE), .groups = \"drop\")\n  da_sf &lt;- da_sf %&gt;%\n    left_join(agg, by = \"._da_row\") %&gt;%\n    mutate(green_area_m2 = replace_na(green_area_m2, 0))\n  da_sf$._da_row &lt;- NULL\n\n  # Green area per 1000 people: (m²) * 1000 / population\n  pop &lt;- da_sf$population\n  pop[!is.finite(pop) | pop &lt;= 0] &lt;- NA\n  da_sf$green_area_per_1000 &lt;- (da_sf$green_area_m2 * 1000) / pop\n  da_sf$green_area_per_1000[!is.finite(da_sf$green_area_per_1000)] &lt;- NA_real_\n\n  da_sf\n}\n\n# Fetch greenspace polygons by OSM type for toggleable map layers (WGS84).\n# Includes both osm_polygons and osm_multipolygons (e.g. natural=wood) so nothing is missed.\nfetch_greenspace_layers &lt;- function(bbox) {\n  bbox_vec &lt;- c(bbox[\"x\", \"min\"], bbox[\"y\", \"min\"], bbox[\"x\", \"max\"], bbox[\"y\", \"max\"])\n  out &lt;- list()\n  for (f in OSM_GREEN_FEATURES) {\n    label &lt;- paste0(f$key, \": \", f$value)\n    poly &lt;- tryCatch({\n      od &lt;- opq(bbox_vec) %&gt;% add_osm_feature(key = f$key, value = f$value) %&gt;% osmdata_sf()\n      polys &lt;- od$osm_polygons\n      multi &lt;- od$osm_multipolygons\n      if (!is.null(multi) && nrow(multi) &gt; 0) {\n        multi &lt;- st_cast(multi %&gt;% select(geometry), \"POLYGON\", warn = FALSE)\n        if (is.null(polys) || nrow(polys) == 0) polys &lt;- multi else polys &lt;- rbind(polys %&gt;% select(geometry), multi)\n      }\n      if (is.null(polys) || nrow(polys) == 0) NULL else (polys %&gt;% select(geometry))\n    }, error = function(e) {\n      warn_if_overpass_error(e)\n      NULL\n    })\n    if (!is.null(poly) && nrow(poly) &gt; 0) {\n      if (!all(st_is_valid(poly))) poly &lt;- st_make_valid(poly)\n      out[[label]] &lt;- list(group = label, sf = poly)\n    }\n  }\n  out\n}\n\n# Fetch all greenspace, merge (union) into one layer so overlaps are single polygons (WGS84).\n# Use for display so the map shows one merged layer with no double-drawn overlap.\n# Uses GeoPackage cache by bbox UID when available; otherwise fetches OSM and caches result.\nfetch_greenspace_merged &lt;- function(bbox, city = NULL) {\n  cached &lt;- get_cached_greenspace(bbox)\n  if (!is.null(cached) && nrow(cached) &gt; 0) {\n    message(\"Using cached merged greenspace for this bbox.\")\n    return(cached)\n  }\n  layers &lt;- fetch_greenspace_layers(bbox)\n  if (length(layers) == 0) return(NULL)\n  all_sf &lt;- do.call(rbind, lapply(layers, function(x) x$sf))\n  if (nrow(all_sf) == 0) return(NULL)\n  if (!all(st_is_valid(all_sf))) all_sf &lt;- st_make_valid(all_sf)\n  utm_zone &lt;- floor((bbox[\"x\", \"min\"] + bbox[\"x\", \"max\"]) / 2 + 180) %/% 6 + 1\n  if (bbox[\"y\", \"min\"] + bbox[\"y\", \"max\"] &gt; 0) {\n    crs_planar &lt;- st_crs(paste0(\"+proj=utm +zone=\", utm_zone, \" +datum=WGS84 +units=m +no_defs\"))\n  } else {\n    crs_planar &lt;- st_crs(paste0(\"+proj=utm +zone=\", utm_zone, \" +south +datum=WGS84 +units=m +no_defs\"))\n  }\n  planar &lt;- st_transform(all_sf, crs_planar)\n  if (!all(st_is_valid(planar))) planar &lt;- st_make_valid(planar)\n  geom &lt;- st_geometry(planar)\n  merged &lt;- tryCatch(\n    st_union(geom),\n    error = function(e) {\n      message(\"fetch_greenspace_merged: st_union failed (\", conditionMessage(e), \"); trying buffer(0)...\")\n      fixed &lt;- tryCatch(st_buffer(planar, 0), error = function(e2) planar)\n      if (!all(st_is_valid(fixed))) fixed &lt;- st_make_valid(fixed)\n      tryCatch(\n        st_union(st_geometry(fixed)),\n        error = function(e2) {\n          message(\"fetch_greenspace_merged: trying chunked union...\")\n          n &lt;- nrow(fixed)\n          chunk &lt;- 200L\n          idx &lt;- seq_len(n)\n          chunks &lt;- split(idx, ceiling(idx / chunk))\n          parts &lt;- lapply(chunks, function(i) st_union(st_geometry(fixed[i, ])))\n          Reduce(st_union, parts)\n        }\n      )\n    }\n  )\n  if (length(merged) &gt; 1) merged &lt;- st_union(merged)\n  merged_sf &lt;- st_sf(geometry = merged, crs = crs_planar)\n  merged_wgs84 &lt;- st_transform(merged_sf, st_crs(all_sf))\n  result &lt;- st_sf(geometry = st_geometry(merged_wgs84), crs = st_crs(all_sf))\n  set_cached_greenspace(bbox, result, city = city)\n  result\n}"
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/index.html#script-process-green",
    "href": "projects/Leaflet_Mapping_Tool/index.html#script-process-green",
    "title": "Leaflet Mapping Tool",
    "section": "process_green.R",
    "text": "process_green.R\nWhat it does: Fetches OSM green-space data (parks, playgrounds, nature reserves) for a given bbox and returns a list with green_status (Limited/Good/Great by count), plus sf layers for parks, playgrounds, and nature (as centroids for markers). Used for green-status summaries or optional marker layers; create_bivariate_map uses process_green_da for per-DA green area.\n\n\nCode\n#\n# OSM green space data and study-area green access status (parks, playgrounds, nature reserves).\n# Input: bbox (matrix with rownames \"x\",\"y\" and colnames \"min\",\"max\").\n# Returns: list with green_status (Limited/Good/Great), parks_sf, playgrounds_sf, etc.\n# Note: create_bivariate_map uses process_green_da.R and fetch_greenspace_* only; this script is for other use (e.g. green-status summaries or marker layers).\n#\n\nlibrary(sf)\nlibrary(dplyr)\nlibrary(osmdata)\n\nprocess_green &lt;- function(bbox) {\n  if (is.null(bbox) || !identical(rownames(bbox), c(\"x\", \"y\"))) {\n    return(list(green_status = NA_character_, parks_sf = NULL, playgrounds_sf = NULL))\n  }\n  bbox_vec &lt;- c(bbox[\"x\", \"min\"], bbox[\"y\", \"min\"], bbox[\"x\", \"max\"], bbox[\"y\", \"max\"])\n\n  # Parks (polygons → centroids for markers)\n  parks &lt;- tryCatch({\n    opq(bbox_vec) %&gt;%\n      add_osm_feature(key = \"leisure\", value = \"park\") %&gt;%\n      osmdata_sf() %&gt;%\n      .$osm_polygons\n  }, error = function(e) NULL)\n  parks_pts &lt;- if (!is.null(parks) && nrow(parks) &gt; 0) st_centroid(parks) else NULL\n\n  # Playgrounds\n  playgrounds &lt;- tryCatch({\n    opq(bbox_vec) %&gt;%\n      add_osm_feature(key = \"leisure\", value = \"playground\") %&gt;%\n      osmdata_sf()\n  }, error = function(e) NULL)\n  play_poly &lt;- if (!is.null(playgrounds$osm_polygons) && nrow(playgrounds$osm_polygons) &gt; 0) playgrounds$osm_polygons else NULL\n  play_pts  &lt;- if (!is.null(playgrounds$osm_points) && nrow(playgrounds$osm_points) &gt; 0) playgrounds$osm_points else NULL\n  playgrounds_sf &lt;- if (!is.null(play_poly)) st_centroid(play_poly) else play_pts\n\n  # Nature reserves (polygons → centroids)\n  nature &lt;- tryCatch({\n    opq(bbox_vec) %&gt;%\n      add_osm_feature(key = \"leisure\", value = \"nature_reserve\") %&gt;%\n      osmdata_sf() %&gt;%\n      .$osm_polygons\n  }, error = function(e) NULL)\n  nature_pts &lt;- if (!is.null(nature) && nrow(nature) &gt; 0) st_centroid(nature) else NULL\n\n  # Green access status: count of park + playground + nature features\n  n_parks &lt;- if (is.null(parks_pts)) 0 else nrow(parks_pts)\n  n_play  &lt;- if (is.null(playgrounds_sf)) 0 else nrow(playgrounds_sf)\n  n_nat   &lt;- if (is.null(nature_pts)) 0 else nrow(nature_pts)\n  total_green &lt;- n_parks + n_play + n_nat\n\n  # Thresholds from map_config (or defaults if not loaded)\n  th &lt;- if (exists(\"map_config\")) map_config$green$thresholds else c(great = 20L, good = 5L, limited = 1L)\n  green_status &lt;- dplyr::case_when(\n    total_green &gt;= th[\"great\"]   ~ \"Great\",\n    total_green &gt;= th[\"good\"]   ~ \"Good\",\n    total_green &gt;= th[\"limited\"] ~ \"Limited\",\n    TRUE                         ~ \"None\"\n  )\n\n  list(\n    green_status = green_status,\n    parks_sf = parks_pts,\n    playgrounds_sf = playgrounds_sf,\n    nature_reserves_sf = nature_pts,\n    n_parks = n_parks,\n    n_playgrounds = n_play,\n    n_nature = n_nat\n  )\n}"
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/index.html#script-build-ses-index",
    "href": "projects/Leaflet_Mapping_Tool/index.html#script-build-ses-index",
    "title": "Leaflet Mapping Tool",
    "section": "build_ses_index.R",
    "text": "build_ses_index.R\nWhat it does: Builds the income/SES GeoPackage from Statistics Canada 2021 Census and DA boundaries. Reads census CSV(s) and boundary shapefiles from data/raw/, joins on DA code, computes median household income and optional SES index (mean ± SD bands), and writes data/ses_index.gpkg. Supports test mode (one region, limited rows). Run after fetch_statcan_census.py (or equivalent) has downloaded and extracted census and boundaries.\n\n\nCode\n#\n# Build SES (socioeconomic status) index from Statistics Canada 2021 Census\n# and DA boundaries. Reads data/raw/ census CSV and data/raw/da_boundaries/,\n# joins, computes index, writes data/ses_index.gpkg.\n#\n# Requires: tidyverse, sf. Run after scripts/fetch_statcan_census.py.\n#\n# Test mode (faster for debugging): set env var SES_INDEX_TEST=true before sourcing,\n#   or in R: Sys.setenv(SES_INDEX_TEST = \"true\"); source(\"R/build_ses_index.R\")\n#   Uses 1 region only, limits census rows to 200k, output: ses_index_test.gpkg\n#\n\nlibrary(tidyverse)\nlibrary(sf)\nif (dir.exists(\"R\")) source(\"R/map_config.R\") else source(\"map_config.R\")\n\n# Test mode: 1 region, limit rows, write to ses_index_test.gpkg\nTEST_MODE &lt;- tolower(Sys.getenv(\"SES_INDEX_TEST\", \"\")) %in% c(\"true\", \"1\", \"yes\")\nN_MAX_CENSUS &lt;- if (TEST_MODE) 200000L else Inf\n\n# Paths (relative to project root when run from project root)\nPROJECT_ROOT &lt;- if (dir.exists(\"data\")) \".\" else if (dir.exists(\"../data\")) \"..\" else stop(\"Run from project root\")\nRAW_DIR     &lt;- file.path(PROJECT_ROOT, \"data\", \"raw\")\nBOUNDARY_DIR &lt;- file.path(RAW_DIR, \"da_boundaries\")\nOUT_GPKG    &lt;- file.path(PROJECT_ROOT, \"data\", if (TEST_MODE) \"ses_index_test.gpkg\" else \"ses_index.gpkg\")\n\n# Find boundary shapefile: prefer cartographic (lda_000b*, land-only, no water) over digital (lda_000a*)\nboundary_files &lt;- list.files(BOUNDARY_DIR, pattern = \"\\\\.shp$\", full.names = TRUE, recursive = TRUE)\nif (length(boundary_files) == 0) stop(\"No .shp found in \", BOUNDARY_DIR, \". Run fetch_statcan_census.py --boundaries first.\")\ncarto &lt;- boundary_files[grepl(\"000b21|cartographic\", basename(boundary_files), ignore.case = TRUE)]\nboundary_path &lt;- if (length(carto) &gt; 0) carto[1] else boundary_files[1]\nif (length(carto) == 0) message(\"Using digital boundaries. For land-only (no water) use cartographic: python scripts/fetch_statcan_census.py --boundaries (downloads lda_000b21a_e).\")\n\n# Find census CSV: prefer census_profile_*.csv (after Python extract), or *CSV_data*.csv in raw/ subdirs\ncensus_files &lt;- list.files(RAW_DIR, pattern = \"census_profile.*\\\\.csv$\", full.names = TRUE)\n# Skip files that are actually ZIPs (StatCan returns zip with .csv name)\ncensus_files &lt;- census_files[vapply(census_files, function(f) {\n  h &lt;- readBin(f, \"raw\", n = 2)\n  !identical(h, charToRaw(\"PK\"))\n}, logical(1))]\nif (length(census_files) == 0) {\n  census_files &lt;- list.files(RAW_DIR, pattern = \".*CSV_data.*\\\\.csv$\", full.names = TRUE, recursive = TRUE)\n  census_files &lt;- census_files[vapply(census_files, function(f) {\n    h &lt;- readBin(f, \"raw\", n = 2)\n    !identical(h, charToRaw(\"PK\"))\n  }, logical(1))]\n}\nif (length(census_files) == 0) {\n  stop(\"No valid census CSV in \", RAW_DIR, \". StatCan returns ZIP: re-run fetch_statcan_census.py to download and extract, or extract the ZIP and place the inner CSV in data/raw/.\")\n}\nif (TEST_MODE) {\n  census_files &lt;- census_files[1]\n  message(\"TEST MODE: using 1 region only, n_max = \", N_MAX_CENSUS, \", output: \", basename(OUT_GPKG))\n}\nmessage(\"Using census file(s): \", paste(basename(census_files), collapse = \", \"))\n\nmessage(\"Reading boundaries from: \", boundary_path)\nboundaries &lt;- st_read(boundary_path, quiet = TRUE) %&gt;%\n  st_transform(4326)\n\nid_col &lt;- intersect(c(\"DGUID\", \"DAUID\", \"dguid\", \"dauid\"), names(boundaries))[1]\nif (is.na(id_col)) id_col &lt;- names(boundaries)[1]\nmessage(\"Using geography ID column: \", id_col)\n\n# StatCan 2021 Census Profile: 1 = Total population; 243 = Median total income of household in 2020 ($), value in C1_COUNT_TOTAL\n# If your CSV uses a different ID, set INCOME_CHAR_ID: Sys.setenv(INCOME_CHAR_ID = \"2151\"); source(\"R/build_ses_index.R\")\nINCOME_CHAR_ID &lt;- as.character(Sys.getenv(\"INCOME_CHAR_ID\", \"243\"))\n\nread_census_safe &lt;- function(path, n_max = Inf) {\n  suppressMessages({\n    d &lt;- read_csv(path, show_col_types = FALSE, guess_max = 10000, n_max = n_max)\n  })\n  d\n}\n\ncensus_list &lt;- lapply(census_files, function(f) read_census_safe(f, n_max = N_MAX_CENSUS))\ncensus_raw  &lt;- bind_rows(census_list)\n\n# Detect long vs wide format\nis_long &lt;- all(c(\"GEO_LEVEL\", \"CHARACTERISTIC_ID\", \"DGUID\") %in% names(census_raw)) &&\n  (\"C10_RATE_TOTAL\" %in% names(census_raw) || \"C1_COUNT_TOTAL\" %in% names(census_raw))\n\nif (is_long) {\n  message(\"Detected long-format Census Profile; filtering to DAs, population and median household income.\")\n  has_count &lt;- \"C1_COUNT_TOTAL\" %in% names(census_raw)\n  has_rate  &lt;- \"C10_RATE_TOTAL\" %in% names(census_raw)\n  # Median household income may be in C1_COUNT_TOTAL (dollar value) or a rate column depending on product\n  alt_geo_col &lt;- intersect(c(\"ALT_GEO_CODE\", \"ALT GEO CODE\"), names(census_raw))[1]\n  if (is.na(alt_geo_col)) alt_geo_col &lt;- names(census_raw)[grep(\"ALT.*GEO|GEO_CODE\", names(census_raw), ignore.case = TRUE)][1]\n  char_ids &lt;- c(\"1\", INCOME_CHAR_ID)\n  census_da &lt;- census_raw %&gt;%\n    filter(GEO_LEVEL == \"Dissemination area\", as.character(CHARACTERISTIC_ID) %in% char_ids) %&gt;%\n    mutate(\n      dguid_join = as.character(trimws(DGUID)),\n      dauid_join = if (!is.na(alt_geo_col)) as.character(trimws(.data[[alt_geo_col]])) else NA_character_,\n      value_num = suppressWarnings(\n        if (has_count && has_rate) {\n          if_else(as.character(CHARACTERISTIC_ID) == \"1\", as.numeric(.data[[\"C1_COUNT_TOTAL\"]]), as.numeric(.data[[\"C1_COUNT_TOTAL\"]]))\n        } else if (has_count) {\n          as.numeric(.data[[\"C1_COUNT_TOTAL\"]])\n        } else {\n          as.numeric(.data[[\"C10_RATE_TOTAL\"]])\n        }\n      )\n    ) %&gt;%\n    filter(!is.na(dguid_join), dguid_join != \"\")\n  census_slim &lt;- census_da %&gt;%\n    group_by(dguid_join) %&gt;%\n    mutate(dauid_join = first(dauid_join)) %&gt;%\n    ungroup() %&gt;%\n    select(dguid_join, dauid_join, CHARACTERISTIC_ID, value_num) %&gt;%\n    tidyr::pivot_wider(names_from = CHARACTERISTIC_ID, values_from = value_num, names_prefix = \"char_\") %&gt;%\n    mutate(\n      population = if (\"char_1\" %in% names(.)) suppressWarnings(as.numeric(.data[[\"char_1\"]])) else NA_real_,\n      income_raw = if (paste0(\"char_\", INCOME_CHAR_ID) %in% names(.)) suppressWarnings(as.numeric(.data[[paste0(\"char_\", INCOME_CHAR_ID)]])) else NA_real_\n    ) %&gt;%\n    select(dguid_join, dauid_join, population, income_raw)\n  if (all(is.na(census_slim$income_raw))) {\n    stop(\"Median household income (CHARACTERISTIC_ID \", INCOME_CHAR_ID, \") not found or all NA. Confirm CHARACTERISTIC_ID in your Census Profile (e.g. 243); set INCOME_CHAR_ID if different.\")\n  }\n  if (all(is.na(census_slim$dauid_join)) || all(trimws(census_slim$dauid_join) == \"\", na.rm = TRUE)) {\n    census_slim &lt;- census_slim %&gt;% mutate(dauid_join = str_sub(dguid_join, -8, -1))\n  }\n  census_slim &lt;- census_slim %&gt;%\n    mutate(dauid_join_raw = trimws(replace_na(dauid_join, \"\")),\n           dauid_join = str_pad(dauid_join_raw, width = 8, side = \"left\", pad = \"0\"))\n} else {\n  census_id_col &lt;- intersect(c(\"DGUID\", \"GEO_CODE\", \"dguid\"), names(census_raw))[1]\n  if (is.na(census_id_col)) census_id_col &lt;- names(census_raw)[1]\n  if (\"GEO_LEVEL\" %in% names(census_raw)) {\n    census_da &lt;- census_raw %&gt;% filter(GEO_LEVEL == 4 | GEO_LEVEL == 5 | GEO_LEVEL == \"Dissemination area\")\n  } else {\n    census_da &lt;- census_raw\n  }\n  find_col &lt;- function(df, patterns) {\n    nms &lt;- names(df)\n    for (p in patterns) {\n      idx &lt;- grep(p, nms, ignore.case = TRUE)\n      if (length(idx) &gt; 0) return(nms[idx[1]])\n    }\n    NA_character_\n  }\n  col_income &lt;- find_col(census_da, c(\"Median total income of household\", \"Median household income\", \"Median total income\", \"Median income\"))\n  cols_keep &lt;- unique(c(census_id_col, col_income))\n  cols_keep &lt;- intersect(cols_keep, names(census_da))\n  if (length(cols_keep) &lt; 2) cols_keep &lt;- names(census_da)[seq_len(min(3, ncol(census_da)))]\n  census_slim &lt;- census_da %&gt;%\n    select(any_of(cols_keep)) %&gt;%\n    mutate(dguid_join = as.character(.data[[census_id_col]]))\n  for (c in setdiff(cols_keep, census_id_col)) {\n    if (c %in% names(census_slim)) census_slim[[c]] &lt;- suppressWarnings(as.numeric(census_slim[[c]]))\n  }\n  census_slim &lt;- census_slim %&gt;% filter(!is.na(dguid_join) & dguid_join != \"\")\n  inc_col &lt;- if (col_income %in% names(census_slim)) col_income else names(census_slim)[sapply(census_slim, is.numeric)][1]\n  census_slim &lt;- census_slim %&gt;%\n    mutate(\n      dauid_join = NA_character_,\n      dauid_join_raw = NA_character_,\n      population = NA_real_,\n      income_raw = if (!is.na(inc_col)) .data[[inc_col]] else NA_real_\n    ) %&gt;%\n    select(dguid_join, dauid_join, dauid_join_raw, population, income_raw)\n}\n\n# Income: use raw median household income ($). Classify by study-area mean ± SD (same logic as before).\n# Keep income_raw as-is; bivariate map uses it for ±1 SD Low/Med/High. idx_st is the 5-category status (Lowest/.../Highest) from map_config for reference.\nref &lt;- census_slim %&gt;%\n  summarise(\n    m_inc = mean(income_raw, na.rm = TRUE),\n    s_inc = sd(income_raw, na.rm = TRUE)\n  )\nref &lt;- ref %&gt;% mutate(s_inc = replace_na(s_inc, 0))\nif (ref$s_inc == 0) ref$s_inc &lt;- 1e-9\n\nlab &lt;- map_config$ses$labels\nm   &lt;- map_config$ses$sd_multipliers\ncensus_slim &lt;- census_slim %&gt;%\n  mutate(\n    ses_idx = income_raw,\n    idx_st = case_when(\n      is.na(income_raw) ~ NA_character_,\n      income_raw &lt; ref$m_inc + m[1] * ref$s_inc ~ lab[1],\n      income_raw &lt; ref$m_inc + m[2] * ref$s_inc ~ lab[2],\n      income_raw &lt;= ref$m_inc + m[3] * ref$s_inc ~ lab[3],\n      income_raw &lt;= ref$m_inc + m[4] * ref$s_inc ~ lab[4],\n      TRUE ~ lab[5]\n    )\n  )\n\n# Join to boundaries. Per StatCan DGUID definition (92F0138M): for DA, DGUID = VVVV+T+SSSS+DAUID\n# (9 fixed chars + 8-char DAUID). Shapefile .dbf often truncates DGUID to 10 chars, so full-DGUID\n# join fails; use last 8 of census DGUID = boundary DAUID as the canonical fallback.\n# https://www150.statcan.gc.ca/n1/pub/92f0138m/92f0138m2019001-eng.htm\nboundary_has_dguid &lt;- \"DGUID\" %in% names(boundaries)\nboundary_has_dauid &lt;- \"DAUID\" %in% names(boundaries)\ncensus_slim$dguid_suffix8 &lt;- str_sub(census_slim$dguid_join, -8, -1)  # last 8 = DAUID per StatCan\n\nses_sf &lt;- NULL\nif (boundary_has_dguid) {\n  ses_sf &lt;- boundaries %&gt;%\n    mutate(dguid_join = as.character(DGUID)) %&gt;%\n    inner_join(census_slim %&gt;% select(dguid_join, population, income_raw, ses_idx, idx_st),\n               by = \"dguid_join\")\n}\nif (is.null(ses_sf) || nrow(ses_sf) == 0 && boundary_has_dauid) {\n  message(\"DGUID join produced 0 rows; trying last 8 of DGUID = DAUID (per StatCan 92F0138M).\")\n  bbox_join &lt;- boundaries %&gt;%\n    mutate(dauid_8 = str_pad(trimws(as.character(DAUID)), width = 8, side = \"left\", pad = \"0\"))\n  ses_sf &lt;- bbox_join %&gt;%\n    inner_join(census_slim %&gt;% select(dauid_8 = dguid_suffix8, population, income_raw, ses_idx, idx_st),\n               by = \"dauid_8\") %&gt;%\n    select(-dauid_8)\n}\nif (is.null(ses_sf) || nrow(ses_sf) == 0 && boundary_has_dauid && \"dauid_join\" %in% names(census_slim)) {\n  message(\"DAUID-from-DGUID join had 0 rows; trying census ALT_GEO_CODE as DAUID.\")\n  bbox_join &lt;- boundaries %&gt;%\n    mutate(dauid_join = str_pad(trimws(as.character(DAUID)), width = 8, side = \"left\", pad = \"0\"))\n  ses_sf &lt;- bbox_join %&gt;%\n    inner_join(census_slim %&gt;% select(dauid_join, population, income_raw, ses_idx, idx_st),\n               by = \"dauid_join\")\n  if (nrow(ses_sf) == 0 && \"dauid_join_raw\" %in% names(census_slim)) {\n    message(\"Padded DAUID join had 0 rows; trying raw DAUID (no zero-pad).\")\n    bbox_join2 &lt;- boundaries %&gt;% mutate(dauid_join = trimws(as.character(DAUID)))\n    census_join2 &lt;- census_slim %&gt;% filter(dauid_join_raw != \"\")\n    if (nrow(census_join2) &gt; 0) {\n      ses_sf &lt;- bbox_join2 %&gt;%\n        inner_join(census_join2 %&gt;% select(dauid_join = dauid_join_raw, population, income_raw, ses_idx, idx_st),\n                   by = \"dauid_join\")\n    }\n  }\n}\n\nif (is.null(ses_sf) || nrow(ses_sf) == 0) {\n  # One more try: match last 8 chars of census DGUID to any boundary ID-like column (e.g. DAUID with different name)\n  id_like &lt;- names(boundaries)[grepl(\"UID|GUID|ID\", names(boundaries), ignore.case = TRUE)]\n  for (bcol in id_like) {\n    if (!bcol %in% names(boundaries)) next\n    bvals &lt;- trimws(as.character(boundaries[[bcol]]))\n    if (!all(nchar(bvals) &gt;= 6, na.rm = TRUE)) next\n    try_join &lt;- boundaries %&gt;%\n      mutate(join_try = .data[[bcol]]) %&gt;%\n      mutate(join_try = trimws(as.character(join_try))) %&gt;%\n      inner_join(census_slim %&gt;% select(join_try = dguid_suffix8, population, income_raw, ses_idx, idx_st), by = \"join_try\")\n    if (nrow(try_join) &gt; 0) {\n      ses_sf &lt;- try_join %&gt;% select(-join_try)\n      message(\"Join succeeded using census DGUID (last 8 chars) = boundary '\", bcol, \"'.\")\n      break\n    }\n  }\n}\n\nif (is.null(ses_sf) || nrow(ses_sf) == 0) {\n  # Detect ADA vs DA: census is DA (DGUID schema 0512); boundaries may be ADA (schema 0516, has ADAUID, no DAUID)\n  boundary_dguid_sample &lt;- if (\"DGUID\" %in% names(boundaries)) head(trimws(as.character(boundaries$DGUID)), 1) else \"\"\n  census_dguid_sample   &lt;- head(census_slim$dguid_join, 1)\n  boundary_is_ada &lt;- (\"ADAUID\" %in% names(boundaries) && !(\"DAUID\" %in% names(boundaries))) ||\n    (nchar(boundary_dguid_sample) &gt;= 9 && grepl(\"0516\", substr(boundary_dguid_sample, 1, 9)))\n  census_is_da   &lt;- nchar(census_dguid_sample) &gt;= 9 && grepl(\"0512\", substr(census_dguid_sample, 1, 9))\n  if (boundary_is_ada && census_is_da) {\n    stop(\n      \"Boundary file is Aggregate Dissemination Area (ADA), but census data is Dissemination Area (DA). \",\n      \"You need DA boundaries. Options: (1) Re-download boundaries: python scripts/fetch_statcan_census.py --boundaries \",\n      \"(then replace contents of data/raw/da_boundaries/ with the new DA shapefile), or \",\n      \"(2) Download the Dissemination Area boundary file (lda_000b21a_e.zip cartographic recommended) from StatCan catalogue 92-169-X2021001 \",\n      \"and extract it into data/raw/da_boundaries/.\"\n    )\n  }\n  message(\"=== Join failed. Diagnostics ===\")\n  message(\"Boundary columns: \", paste(names(boundaries), collapse = \", \"))\n  id_like_diag &lt;- names(boundaries)[grepl(\"UID|GUID|ID\", names(boundaries), ignore.case = TRUE)]\n  for (c in id_like_diag) {\n    message(\"  Boundary \", c, \" (first 3): \", paste(head(unique(as.character(boundaries[[c]])), 3), collapse = \" | \"))\n  }\n  message(\"Census dguid_join (first 3): \", paste(head(unique(census_slim$dguid_join), 3), collapse = \" | \"))\n  message(\"Census dauid_join (first 3): \", paste(head(unique(census_slim$dauid_join), 3), collapse = \" | \"))\n  if (\"dauid_join_raw\" %in% names(census_slim)) message(\"Census dauid_join_raw (first 3): \", paste(head(unique(census_slim$dauid_join_raw), 3), collapse = \" | \"))\n  message(\"Census dguid_suffix8 (first 3): \", paste(head(unique(census_slim$dguid_suffix8), 3), collapse = \" | \"))\n  stop(\"Join produced 0 rows. Census and boundary IDs do not match. See diagnostics above.\")\n}\n\nmessage(\"Writing \", nrow(ses_sf), \" DAs to \", OUT_GPKG)\ndir.create(dirname(OUT_GPKG), recursive = TRUE, showWarnings = FALSE)\nst_write(ses_sf, OUT_GPKG, delete_dsn = TRUE, quiet = TRUE)\nmessage(\"Done: \", OUT_GPKG)\nif (TEST_MODE) message(\"(Test mode. For full Canada, unset SES_INDEX_TEST and re-run.)\")"
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/index.html#script-process-bbox",
    "href": "projects/Leaflet_Mapping_Tool/index.html#script-process-bbox",
    "title": "Leaflet Mapping Tool",
    "section": "process_bbox.R",
    "text": "process_bbox.R\nWhat it does: Resolves the study-area bounding box (from city name via osmdata::getbb, custom_bbox, or amalgamation_cities_list), optionally expands it by custom_distance_km, clips the income GeoPackage to the bbox, and writes clipped polygons to data/clipped_income/. Caches clipped layer by bbox UID in data/cache.gpkg. Returns bbox matrix and paths needed by create_bivariate_map.\n\n\nCode\n# Process bounding box and clip income layer to study area.\n# Uses income_gpkg (path to income GeoPackage, e.g. data/ses_index.gpkg or data/income.gpkg), writes to data/clipped_income/.\n# Caches clipped layer by bbox UID in data/cache.gpkg (layer clipped_income) when available.\n\nlibrary(sf)\nlibrary(geosphere)\nlibrary(osmdata)\nlibrary(stringr)\nif (dir.exists(\"R\")) source(\"R/cache_utils.R\") else source(\"cache_utils.R\")\n\nprocess_bbox &lt;- function(study_area, income_gpkg, amalgamation_cities_list = NULL,\n                         city_pr = NULL, custom_bbox = NULL, custom_distance_km = 0) {\n\n  income_sf &lt;- st_read(income_gpkg, quiet = TRUE)\n  # Fix invalid polygons (e.g. duplicate vertices) so st_intersection doesn't fail\n  if (!all(st_is_valid(income_sf))) {\n    income_sf &lt;- st_make_valid(income_sf)\n  }\n\n  expand_bbox &lt;- function(bbox, distance_km) {\n    lat_expansion &lt;- distance_km / 111.32\n    centroid_lat &lt;- (bbox[\"y\", \"min\"] + bbox[\"y\", \"max\"]) / 2\n    lon_expansion &lt;- distance_km / (111.32 * cos(centroid_lat * pi / 180))\n    matrix(\n      c(\n        bbox[\"x\", \"min\"] - lon_expansion,\n        bbox[\"y\", \"min\"] - lat_expansion,\n        bbox[\"x\", \"max\"] + lon_expansion,\n        bbox[\"y\", \"max\"] + lat_expansion\n      ),\n      nrow = 2,\n      dimnames = list(c(\"x\", \"y\"), c(\"min\", \"max\"))\n    )\n  }\n\n  if (!is.null(custom_bbox)) {\n    if (length(custom_bbox) != 4) stop(\"custom_bbox must be c(xmin, ymin, xmax, ymax)\")\n    bbox &lt;- matrix(\n      c(custom_bbox[1], custom_bbox[2], custom_bbox[3], custom_bbox[4]),\n      nrow = 2,\n      dimnames = list(c(\"x\", \"y\"), c(\"min\", \"max\"))\n    )\n  } else if (!is.null(city_pr)) {\n    stop(\"Processing multiple cities (city_pr) is not supported with automatic bbox saving.\")\n  } else if (!is.null(amalgamation_cities_list)) {\n    bboxes &lt;- lapply(amalgamation_cities_list, getbb)\n    bbox &lt;- matrix(\n      c(\n        min(sapply(bboxes, function(bb) bb[\"x\", \"min\"])),\n        min(sapply(bboxes, function(bb) bb[\"y\", \"min\"])),\n        max(sapply(bboxes, function(bb) bb[\"x\", \"max\"])),\n        max(sapply(bboxes, function(bb) bb[\"y\", \"max\"]))\n      ),\n      nrow = 2,\n      dimnames = list(c(\"x\", \"y\"), c(\"min\", \"max\"))\n    )\n  } else {\n    bbox &lt;- getbb(study_area)\n  }\n\n  if (custom_distance_km &gt; 0) bbox &lt;- expand_bbox(bbox, custom_distance_km)\n\n  assign(\"expanded_bbox\", bbox, envir = .GlobalEnv)\n\n  modified_text &lt;- if (!is.null(study_area)) {\n    str_replace_all(str_extract(study_area, \"^[^,]+\"), \" \", \"_\")\n  } else {\n    \"custom_bbox\"\n  }\n\n  out_dir &lt;- file.path(dirname(income_gpkg), \"clipped_income\")\n  dir.create(out_dir, recursive = TRUE, showWarnings = FALSE)\n  out_path &lt;- file.path(out_dir, paste0(modified_text, \".gpkg\"))\n\n  # Use cached clipped layer for this bbox when available\n  cached_clip &lt;- get_cached_clipped_income(bbox)\n  if (!is.null(cached_clip) && nrow(cached_clip) &gt; 0) {\n    st_write(cached_clip, out_path, delete_dsn = TRUE, quiet = TRUE)\n    message(\"Using cached clipped income for this bbox.\")\n    return(bbox)\n  }\n\n  bbox_polygon &lt;- st_as_sfc(st_bbox(\n    c(xmin = bbox[\"x\", \"min\"], ymin = bbox[\"y\", \"min\"], xmax = bbox[\"x\", \"max\"], ymax = bbox[\"y\", \"max\"]),\n    crs = st_crs(4326)\n  ))\n\n  clipped &lt;- tryCatch(\n    st_intersection(income_sf, bbox_polygon),\n    error = function(e) {\n      # s2 can fail on some boundary geometries; retry using planar (GEOS) intersection\n      message(\"Retrying clip in planar CRS (s2 failed: \", conditionMessage(e), \")\")\n      utm_zone &lt;- floor((bbox[\"x\", \"min\"] + bbox[\"x\", \"max\"]) / 2 + 180) %/% 6 + 1\n      if (bbox[\"y\", \"min\"] + bbox[\"y\", \"max\"] &gt; 0) {\n        crs_planar &lt;- st_crs(paste0(\"+proj=utm +zone=\", utm_zone, \" +datum=WGS84 +units=m +no_defs\"))\n      } else {\n        crs_planar &lt;- st_crs(paste0(\"+proj=utm +zone=\", utm_zone, \" +south +datum=WGS84 +units=m +no_defs\"))\n      }\n      income_planar &lt;- st_transform(income_sf, crs_planar)\n      bbox_planar &lt;- st_transform(bbox_polygon, crs_planar)\n      out &lt;- st_intersection(income_planar, bbox_planar)\n      st_transform(out, st_crs(4326))\n    }\n  )\n  if (nrow(clipped) == 0) warning(\"Bbox does not intersect any income-layer polygons; clipped layer is empty. Use a larger area or different city.\", call. = FALSE)\n\n  st_write(clipped, out_path, delete_dsn = TRUE, quiet = TRUE)\n  set_cached_clipped_income(bbox, clipped)\n\n  return(bbox)\n}"
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/index.html#script-cache-utils",
    "href": "projects/Leaflet_Mapping_Tool/index.html#script-cache-utils",
    "title": "Leaflet Mapping Tool",
    "section": "cache_utils.R",
    "text": "cache_utils.R\nWhat it does: GeoPackage cache utilities for clipped_income, greenspace, and buffered DAs. bbox_to_uid() generates a stable UID from bbox coordinates; ensure_cache() creates the cache file and empty layers; read/write helpers avoid re-fetching OSM or re-clipping when the same bbox (and buffer) has already been processed. Reduces Overpass load and speeds up repeated runs.\n\n\nCode\n#\n# GeoPackage cache: clipped income (by bbox), greenspace (by bbox), buffered DAs (by bbox + buffer_m).\n# Cache path: data/cache.gpkg (layers: clipped_income, greenspace, da_buffers).\n# UID is derived only from bbox coordinates (normalized); city is optional metadata.\n#\n\nlibrary(sf)\nlibrary(dplyr)\n\n# Default cache path (under project data/)\nCACHE_GPKG &lt;- \"data/cache.gpkg\"\nCACHE_PRECISION &lt;- 5L  # decimal places for bbox normalization\n\n#' Generate a stable short UID from a bbox (matrix with x/y min/max or equivalent).\n#' Same bbox always yields the same UID; city/label is not used.\n#' @param bbox 2x2 matrix with rownames \"x\",\"y\" and colnames \"min\",\"max\", or length-4 vector c(xmin, ymin, xmax, ymax)\n#' @return character UID\nbbox_to_uid &lt;- function(bbox) {\n  if (is.matrix(bbox) && all(c(\"x\", \"y\") %in% rownames(bbox)) && all(c(\"min\", \"max\") %in% colnames(bbox))) {\n    vec &lt;- c(bbox[\"x\", \"min\"], bbox[\"y\", \"min\"], bbox[\"x\", \"max\"], bbox[\"y\", \"max\"])\n  } else if (is.vector(bbox) && length(bbox) &gt;= 4) {\n    vec &lt;- bbox[1:4]\n  } else {\n    stop(\"bbox must be a 2x2 matrix (x/y, min/max) or length-4 vector c(xmin, ymin, xmax, ymax)\")\n  }\n  vec &lt;- round(as.numeric(vec), CACHE_PRECISION)\n  key &lt;- paste(vec, collapse = \"_\")\n  if (requireNamespace(\"digest\", quietly = TRUE)) {\n    return(substring(digest::digest(key, algo = \"sha256\"), 1L, 16L))\n  }\n  key &lt;- gsub(\"[^0-9a-zA-Z._-]\", \"\", key)\n  if (nchar(key) &gt; 64) key &lt;- substring(key, 1L, 64L)\n  key\n}\n\n#' Ensure cache file and layers exist (creates empty layers if missing).\nensure_cache &lt;- function(path = CACHE_GPKG) {\n  dir &lt;- dirname(path)\n  if (!dir.exists(dir)) dir.create(dir, recursive = TRUE)\n  wgs &lt;- st_crs(4326)\n  if (!file.exists(path)) {\n    empty_geom &lt;- st_sfc(crs = wgs)\n    empty_green &lt;- st_sf(\n      bbox_uid = character(0),\n      xmin = numeric(0), ymin = numeric(0), xmax = numeric(0), ymax = numeric(0),\n      city = character(0), fetched_at = character(0),\n      geometry = empty_geom\n    )\n    st_write(empty_green, path, layer = \"greenspace\", quiet = TRUE)\n    empty_buf &lt;- st_sf(\n      bbox_uid = character(0), buffer_m = integer(0), city = character(0),\n      fetched_at = character(0), da_row = integer(0),\n      geometry = empty_geom\n    )\n    st_write(empty_buf, path, layer = \"da_buffers\", quiet = TRUE)\n  }\n  invisible(path)\n}\n\n#' Read cached merged greenspace for a bbox. Returns sf (one row, WGS84) or NULL if miss.\nget_cached_greenspace &lt;- function(bbox, path = CACHE_GPKG) {\n  ensure_cache(path)\n  uid &lt;- bbox_to_uid(bbox)\n  if (!\"greenspace\" %in% st_layers(path)$name) return(NULL)\n  cached &lt;- tryCatch(\n    st_read(path, layer = \"greenspace\", quiet = TRUE),\n    error = function(e) NULL\n  )\n  if (is.null(cached) || nrow(cached) == 0) return(NULL)\n  hit &lt;- cached[cached$bbox_uid == uid, ]\n  if (nrow(hit) == 0) return(NULL)\n  # Geometry column name can be \"geom\" when read from GeoPackage; use st_geometry to avoid select(geometry) failing\n  st_sf(geometry = st_geometry(hit), crs = st_crs(cached))\n}\n\n#' Write merged greenspace to cache (one row per bbox). merged_sf: single geometry, WGS84 preferred.\nset_cached_greenspace &lt;- function(bbox, merged_sf, city = NULL, path = CACHE_GPKG) {\n  ensure_cache(path)\n  uid &lt;- bbox_to_uid(bbox)\n  if (is.matrix(bbox) && all(c(\"x\", \"y\") %in% rownames(bbox)))\n    coords &lt;- c(bbox[\"x\", \"min\"], bbox[\"y\", \"min\"], bbox[\"x\", \"max\"], bbox[\"y\", \"max\"])\n  else\n    coords &lt;- bbox[1:4]\n  merged_sf &lt;- st_transform(merged_sf, 4326)\n  row &lt;- st_sf(\n    bbox_uid = uid,\n    xmin = coords[1], ymin = coords[2], xmax = coords[3], ymax = coords[4],\n    city = as.character(city %||% NA_character_),\n    fetched_at = format(Sys.time(), \"%Y-%m-%dT%H:%M:%SZ\"),\n    geometry = st_geometry(merged_sf),\n    crs = st_crs(4326)\n  )\n  existing &lt;- tryCatch(st_read(path, layer = \"greenspace\", quiet = TRUE), error = function(e) NULL)\n  if (!is.null(existing) && nrow(existing) &gt; 0) {\n    existing &lt;- existing[existing$bbox_uid != uid, ]\n    if (nrow(existing) &gt; 0) {\n      geom_col &lt;- attr(existing, \"sf_column\")\n      if (geom_col != \"geometry\") {\n        names(existing)[names(existing) == geom_col] &lt;- \"geometry\"\n        attr(existing, \"sf_column\") &lt;- \"geometry\"\n      }\n      existing &lt;- existing[, names(row), drop = FALSE]\n      row &lt;- rbind(existing, row)\n    }\n  }\n  st_write(row, path, layer = \"greenspace\", quiet = TRUE, append = FALSE)\n  invisible(NULL)\n}\n\n#' Read cached buffered DAs for (bbox, buffer_m). Returns sf with da_row and geometry (WGS84) or NULL.\nget_cached_buffers &lt;- function(bbox, buffer_m, path = CACHE_GPKG) {\n  ensure_cache(path)\n  uid &lt;- bbox_to_uid(bbox)\n  if (!\"da_buffers\" %in% st_layers(path)$name) return(NULL)\n  cached &lt;- tryCatch(\n    st_read(path, layer = \"da_buffers\", quiet = TRUE),\n    error = function(e) NULL\n  )\n  if (is.null(cached) || nrow(cached) == 0) return(NULL)\n  hit &lt;- cached[cached$bbox_uid == uid & as.integer(cached$buffer_m) == as.integer(buffer_m), ]\n  if (nrow(hit) == 0) return(NULL)\n  # Geometry column name can be \"geom\" when read from GeoPackage\n  st_sf(da_row = hit$da_row, geometry = st_geometry(hit), crs = st_crs(cached))\n}\n\n#' Write buffered DAs to cache. buffers_sf must have ._da_row and geometry (any CRS; stored as WGS84).\nset_cached_buffers &lt;- function(bbox, buffer_m, buffers_sf, city = NULL, path = CACHE_GPKG) {\n  if (!\"._da_row\" %in% names(buffers_sf)) stop(\"buffers_sf must have ._da_row\")\n  ensure_cache(path)\n  uid &lt;- bbox_to_uid(bbox)\n  buf &lt;- buffers_sf %&gt;% select(._da_row)\n  buf$bbox_uid &lt;- uid\n  buf$buffer_m &lt;- as.integer(buffer_m)\n  buf$city &lt;- as.character(city %||% NA_character_)\n  buf$fetched_at &lt;- format(Sys.time(), \"%Y-%m-%dT%H:%M:%SZ\")\n  buf &lt;- buf %&gt;% rename(da_row = ._da_row)\n  buf &lt;- st_transform(buf, 4326)\n  if (attr(buf, \"sf_column\") != \"geometry\") {\n    names(buf)[names(buf) == attr(buf, \"sf_column\")] &lt;- \"geometry\"\n    attr(buf, \"sf_column\") &lt;- \"geometry\"\n  }\n  existing &lt;- tryCatch(st_read(path, layer = \"da_buffers\", quiet = TRUE), error = function(e) NULL)\n  if (!is.null(existing) && nrow(existing) &gt; 0) {\n    existing &lt;- existing[!(existing$bbox_uid == uid & as.integer(existing$buffer_m) == as.integer(buffer_m)), ]\n    if (nrow(existing) &gt; 0) {\n      geom_col &lt;- attr(existing, \"sf_column\")\n      if (geom_col != \"geometry\") {\n        names(existing)[names(existing) == geom_col] &lt;- \"geometry\"\n        attr(existing, \"sf_column\") &lt;- \"geometry\"\n      }\n      common &lt;- intersect(names(existing), names(buf))\n      existing &lt;- existing[, common, drop = FALSE]\n      buf &lt;- buf[, common, drop = FALSE]\n      buf &lt;- rbind(existing, buf)\n    }\n  }\n  st_write(buf, path, layer = \"da_buffers\", quiet = TRUE, append = FALSE)\n  invisible(NULL)\n}\n\n#' Read cached clipped income layer for a bbox. Returns sf (WGS84) or NULL if miss.\nget_cached_clipped_income &lt;- function(bbox, path = CACHE_GPKG) {\n  ensure_cache(path)\n  uid &lt;- bbox_to_uid(bbox)\n  if (!\"clipped_income\" %in% st_layers(path)$name) return(NULL)\n  cached &lt;- tryCatch(\n    st_read(path, layer = \"clipped_income\", quiet = TRUE),\n    error = function(e) NULL\n  )\n  if (is.null(cached) || nrow(cached) == 0) return(NULL)\n  hit &lt;- cached[cached$bbox_uid == uid, ]\n  if (nrow(hit) == 0) return(NULL)\n  hit$bbox_uid &lt;- NULL\n  hit\n}\n\n#' Write clipped income layer to cache (keyed by bbox UID). clipped_sf: output of st_intersection with bbox.\nset_cached_clipped_income &lt;- function(bbox, clipped_sf, path = CACHE_GPKG) {\n  ensure_cache(path)\n  uid &lt;- bbox_to_uid(bbox)\n  clipped_sf$bbox_uid &lt;- uid\n  existing &lt;- NULL\n  if (\"clipped_income\" %in% st_layers(path)$name) {\n    existing &lt;- tryCatch(\n      st_read(path, layer = \"clipped_income\", quiet = TRUE),\n      error = function(e) NULL\n    )\n  }\n  if (!is.null(existing) && nrow(existing) &gt; 0) {\n    existing &lt;- existing[existing$bbox_uid != uid, ]\n    if (nrow(existing) &gt; 0) {\n      geom_col &lt;- attr(existing, \"sf_column\")\n      target_geom &lt;- attr(clipped_sf, \"sf_column\")\n      if (geom_col != target_geom) {\n        names(existing)[names(existing) == geom_col] &lt;- target_geom\n        attr(existing, \"sf_column\") &lt;- target_geom\n      }\n      existing &lt;- existing[, names(clipped_sf), drop = FALSE]\n      clipped_sf &lt;- rbind(existing, clipped_sf)\n    }\n  }\n  st_write(clipped_sf, path, layer = \"clipped_income\", quiet = TRUE, append = FALSE)\n  invisible(NULL)\n}\n\n# Simple %||% for optional default\n`%||%` &lt;- function(x, y) if (is.null(x)) y else x\n\n\n\nSee the Guide in this project for setup and how to run the tool."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Filter by Tech stack, Methodology, or Domain—click a tag to filter. Each project can span multiple areas.\n\n\n\n\n  \n    \n    Clear filters\n  \n  \n  \n  No projects match the selected tags. Clear filters to see all."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "I help public-sector organizations make sense of complex health, spatial, and administrative data by supporting the full lifecycle of analytical and research projects—from problem definition and data quality assessment to analysis, communication, and future‑proof workflow design.\n\n\n\n\nHealth & Spatial Data Analysis — GIS, spatial modelling, administrative data, health indicators\n\nResearch Design & Evidence Synthesis — mixed methods, qualitative/quantitative integration, policy-oriented analysis\n\nData Quality & Governance — validation, documentation, assumptions, reproducibility\n\nProject Coordination & Collaboration — cross-functional work, planning, workflows, stakeholder engagement\n\nCommunication & Digital Enablement — training, documentation, dashboards, user-focused outputs\n\n\n\n\n\nAbout: Background, values, and interdisciplinary approach\n\nWhat I Do: Capability pillars and how I work\n\nProjects: Spatial analysis, automation, research, and data-driven work\nResume: Experience across municipal, federal, and academic environments\n\nWork With Me: How I support teams and consulting clients\n\n\n\n\n\nView all projects — Spatial analysis, automation, research, and data-driven work."
  },
  {
    "objectID": "index.html#kevin-mackay-phd-data-spatial-research-professional",
    "href": "index.html#kevin-mackay-phd-data-spatial-research-professional",
    "title": "Home",
    "section": "",
    "text": "I help public-sector organizations make sense of complex health, spatial, and administrative data by supporting the full lifecycle of analytical and research projects—from problem definition and data quality assessment to analysis, communication, and future‑proof workflow design."
  },
  {
    "objectID": "index.html#what-i-do",
    "href": "index.html#what-i-do",
    "title": "Home",
    "section": "",
    "text": "Health & Spatial Data Analysis — GIS, spatial modelling, administrative data, health indicators\n\nResearch Design & Evidence Synthesis — mixed methods, qualitative/quantitative integration, policy-oriented analysis\n\nData Quality & Governance — validation, documentation, assumptions, reproducibility\n\nProject Coordination & Collaboration — cross-functional work, planning, workflows, stakeholder engagement\n\nCommunication & Digital Enablement — training, documentation, dashboards, user-focused outputs"
  },
  {
    "objectID": "index.html#explore-my-work",
    "href": "index.html#explore-my-work",
    "title": "Home",
    "section": "",
    "text": "About: Background, values, and interdisciplinary approach\n\nWhat I Do: Capability pillars and how I work\n\nProjects: Spatial analysis, automation, research, and data-driven work\nResume: Experience across municipal, federal, and academic environments\n\nWork With Me: How I support teams and consulting clients"
  },
  {
    "objectID": "index.html#featured-project",
    "href": "index.html#featured-project",
    "title": "Home",
    "section": "",
    "text": "View all projects — Spatial analysis, automation, research, and data-driven work."
  },
  {
    "objectID": "projects/example-stats.html",
    "href": "projects/example-stats.html",
    "title": "Example Stats Project",
    "section": "",
    "text": "Placeholder: Describe one statistics or data project — goal, data, methods, and outcome.\nThis example shows where a single project write-up lives. Replace with your own narrative and, if you like, embedded code or figures."
  },
  {
    "objectID": "projects/example-stats.html#overview",
    "href": "projects/example-stats.html#overview",
    "title": "Example Stats Project",
    "section": "",
    "text": "Placeholder: Describe one statistics or data project — goal, data, methods, and outcome.\nThis example shows where a single project write-up lives. Replace with your own narrative and, if you like, embedded code or figures."
  },
  {
    "objectID": "projects/example-stats.html#methods",
    "href": "projects/example-stats.html#methods",
    "title": "Example Stats Project",
    "section": "Methods",
    "text": "Methods\n\nData: Placeholder source (e.g., survey, administrative data).\nTools: R, tidyverse, relevant packages (e.g., broom, infer).\nOutput: Summary tables, plots, or a short report."
  },
  {
    "objectID": "projects/example-stats.html#code-snippet-placeholder",
    "href": "projects/example-stats.html#code-snippet-placeholder",
    "title": "Example Stats Project",
    "section": "Code snippet (placeholder)",
    "text": "Code snippet (placeholder)\nBelow is a minimal R chunk to show how code will render. Use your real analysis when you add content.\n\n\nCode\nlibrary(tibble)\nlibrary(magrittr)\nlibrary(ggplot2)\ntibble(x = 1:20, y = cumsum(rnorm(20))) %&gt;%\n  ggplot(aes(x, y)) +\n  geom_line(linewidth = 1.2) +\n  theme_minimal() +\n  labs(x = \"Step\", y = \"Cumulative value\")\n\n\n\n\n\n\n\n\nFigure 1: Placeholder: replace with your own plot"
  },
  {
    "objectID": "projects/example-stats.html#takeaways",
    "href": "projects/example-stats.html#takeaways",
    "title": "Example Stats Project",
    "section": "Takeaways",
    "text": "Takeaways\nPlaceholder: 2–3 bullet points summarizing what you did and what you learned."
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/vignettes/vignettes.html",
    "href": "projects/Leaflet_Mapping_Tool/vignettes/vignettes.html",
    "title": "Mapping examples",
    "section": "",
    "text": "The bivariate mapping tool turns census-style income data and OpenStreetMap greenspace into Income × green area per 1,000 people at the dissemination area (DA) level. The maps below show how different arguments change what you map and where—from a single city to amalgamated regions, from a tight urban core to a wide rural study area, and from Canada to anywhere you have equivalent data.\nHow this page is built. To keep the page fast and to avoid Overpass API rate limits, the maps are pre-generated and embedded here. The code under each map is the exact call used to create it; you can run it yourself (see Running the examples yourself). We use geographically modest study areas where possible so that generating maps is quick and reliable."
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/vignettes/vignettes.html#why-these-examples",
    "href": "projects/Leaflet_Mapping_Tool/vignettes/vignettes.html#why-these-examples",
    "title": "Mapping examples",
    "section": "",
    "text": "The bivariate mapping tool turns census-style income data and OpenStreetMap greenspace into Income × green area per 1,000 people at the dissemination area (DA) level. The maps below show how different arguments change what you map and where—from a single city to amalgamated regions, from a tight urban core to a wide rural study area, and from Canada to anywhere you have equivalent data.\nHow this page is built. To keep the page fast and to avoid Overpass API rate limits, the maps are pre-generated and embedded here. The code under each map is the exact call used to create it; you can run it yourself (see Running the examples yourself). We use geographically modest study areas where possible so that generating maps is quick and reliable."
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/vignettes/vignettes.html#hamilton-on-default-run",
    "href": "projects/Leaflet_Mapping_Tool/vignettes/vignettes.html#hamilton-on-default-run",
    "title": "Mapping examples",
    "section": "Hamilton, ON — Default run",
    "text": "Hamilton, ON — Default run\nA straightforward run with only the required arguments: study area and path to the income GeoPackage. The tool clips DAs to the city, fetches greenspace for the bbox, and sums green area per DA (inside each DA plus within a 500 m walkable buffer). Good baseline for comparing with the options below.\n\nsource(\"R/create_bivariate_map.R\")\ncreate_bivariate_map(\n  city = \"Hamilton, ON\",\n  income_gpkg = \"data/ses_index.gpkg\"\n)\n# Output: output/maps/Hamilton_bivariate.html"
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/vignettes/vignettes.html#hamilton-expanding-the-study-area",
    "href": "projects/Leaflet_Mapping_Tool/vignettes/vignettes.html#hamilton-expanding-the-study-area",
    "title": "Mapping examples",
    "section": "Hamilton — Expanding the study area",
    "text": "Hamilton — Expanding the study area\nWhen the city boundary is tight, edge DAs can have much of their walkable buffer outside the default bbox, so their green area is undercounted. Use custom_distance_km so the bounding box is expanded by that many kilometres in all directions; the same bbox is used for clipping and for fetching OSM greenspace, so edge DAs get a fair count.\n\n\nCode\ncreate_bivariate_map(\n  city = \"Hamilton, ON\",\n  income_gpkg = \"data/ses_index.gpkg\",\n  custom_distance_km = 5\n)"
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/vignettes/vignettes.html#hamilton-walkable-buffer-and-greenspace-layers",
    "href": "projects/Leaflet_Mapping_Tool/vignettes/vignettes.html#hamilton-walkable-buffer-and-greenspace-layers",
    "title": "Mapping examples",
    "section": "Hamilton — Walkable buffer and greenspace layers",
    "text": "Hamilton — Walkable buffer and greenspace layers\n\ngreen_buffer_m — Walkable distance (m) around each DA. Greenspace is summed inside the DA and within this distance (default 500 m). Increase it for a longer “access radius” (e.g. 1 km); decrease it for a stricter, shorter walk.\nshow_green_layers — If TRUE, the map adds toggleable OSM layers by type (parks, forest, meadow, etc.). Useful for exploring what counts as green; the main metric still uses the merged greenspace layer.\n\n\n\nCode\ncreate_bivariate_map(\n  city = \"Hamilton, ON\",\n  income_gpkg = \"data/ses_index.gpkg\",\n  green_buffer_m = 1000,\n  show_green_layers = TRUE\n)"
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/vignettes/vignettes.html#canadian-variety-urban-suburban-and-smaller-centres",
    "href": "projects/Leaflet_Mapping_Tool/vignettes/vignettes.html#canadian-variety-urban-suburban-and-smaller-centres",
    "title": "Mapping examples",
    "section": "Canadian variety — Urban, suburban, and smaller centres",
    "text": "Canadian variety — Urban, suburban, and smaller centres\nThe same function works across geographies. Here we use one city per map and keep study areas manageable to limit OSM load and rate limits.\nHalifax, NS — Urban core; default buffer and bbox usually suffice.\n\n\nCode\ncreate_bivariate_map(\n  city = \"Halifax, NS\",\n  income_gpkg = \"data/ses_index.gpkg\"\n)\n\n\n\n\n\n\nCharlottetown, PE — Small capital; compact bbox, good for testing and for showing contrast between downtown and lower-density edges.\n\n\nCode\ncreate_bivariate_map(\n  city = \"Charlottetown, PE\",\n  income_gpkg = \"data/ses_index.gpkg\"\n)\n\n\n\n\n\n\nBrandon, MB — Prairie city; mix of urban and suburban DAs. A small expansion can improve edge-DA counts.\n\n\nCode\ncreate_bivariate_map(\n  city = \"Brandon, MB\",\n  income_gpkg = \"data/ses_index.gpkg\",\n  custom_distance_km = 3\n)\n\n\n\n\n\n\nYellowknife, NT — Northern, smaller footprint. Illustrates the tool in a remote context; income and greenspace patterns differ from southern cities.\n\n\nCode\ncreate_bivariate_map(\n  city = \"Yellowknife, NT\",\n  income_gpkg = \"data/ses_index.gpkg\"\n)"
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/vignettes/vignettes.html#custom-bounding-box",
    "href": "projects/Leaflet_Mapping_Tool/vignettes/vignettes.html#custom-bounding-box",
    "title": "Mapping examples",
    "section": "Custom bounding box",
    "text": "Custom bounding box\nWhen the study area isn’t a single named place—e.g. a corridor, a watershed, rural or remote areas (“middle of nowhere”), or a geologically distinct region where people live—use custom_bbox: c(xmin, ymin, xmax, ymax) in WGS84. The tool uses this bbox for clipping and for fetching OSM; city is still used only for the output filename (use a short label that describes the area).\nExample: Drumheller badlands, Alberta. The Red Deer River valley and badlands have DAs (town of Drumheller and surrounding area) but the interesting unit is the valley, not “Hamilton” or a generic city. A custom bbox focuses the map on that landscape.\n\n\nCode\ncreate_bivariate_map(\n  city = \"Drumheller, AB\",\n  income_gpkg = \"data/ses_index.gpkg\",\n  custom_bbox = c(-112.82, 51.42, -112.55, 51.52)  # Red Deer River valley / badlands\n)"
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/vignettes/vignettes.html#beyond-canada-adapting-the-tool",
    "href": "projects/Leaflet_Mapping_Tool/vignettes/vignettes.html#beyond-canada-adapting-the-tool",
    "title": "Mapping examples",
    "section": "Beyond Canada — Adapting the tool",
    "text": "Beyond Canada — Adapting the tool\nThe mapping pipeline is location-agnostic: it needs (1) a study area that osmdata::getbb() can resolve (or a custom_bbox) and (2) a GeoPackage of small-area polygons with population and an income variable (e.g. median household income or a composite index). If you have administrative data for another country, you can build an equivalent income_gpkg and run the same workflow.\nExample (conceptual): Dublin, Ireland. Replace Canadian census with Irish data; keep DA-like boundaries and population. Then:\n\n\nCode\n# Conceptual: income_gpkg would point to your Irish (or other) data.\n# create_bivariate_map(\n#   city = \"Dublin, Ireland\",\n#   income_gpkg = \"data/ireland_income.gpkg\",\n#   green_buffer_m = 500\n# )\n\n\nOpenStreetMap greenspace is global, so the only change is the demographic source. The tool supports any location with equivalent demographic and boundary data.*"
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/vignettes/vignettes.html#summary-of-arguments",
    "href": "projects/Leaflet_Mapping_Tool/vignettes/vignettes.html#summary-of-arguments",
    "title": "Mapping examples",
    "section": "Summary of arguments",
    "text": "Summary of arguments\n\n\n\n\n\n\n\n\nArgument\nRequired\nPurpose\n\n\n\n\ncity\nYes\nLabel for the output filename (e.g. Hamilton_bivariate.html). When custom_bbox is not set, also used to get the study-area bbox via OSM (e.g. \"Hamilton, ON\").\n\n\nincome_gpkg\nYes\nPath to GeoPackage with DA polygons, population, and an income variable (e.g. income_raw). Typically from build_ses_index.R → data/ses_index.gpkg.\n\n\ncustom_bbox\nNo\nOverride the study-area bbox: c(xmin, ymin, xmax, ymax) in WGS84. Use for corridors, watersheds, rural/remote areas, or regions without a single place name.\n\n\ncustom_distance_km\nNo\nExpand the bbox by this many km in all directions (default 0). Helps edge DAs get full greenspace counts.\n\n\ngreen_buffer_m\nNo\nWalkable distance (m) around each DA; greenspace is summed inside the DA and within this distance (default 500).\n\n\nshow_green_layers\nNo\nIf TRUE, add toggleable map layers by OSM greenspace type (parks, forest, etc.).\n\n\namalgamation_cities_list\nNo\nList of place names to build a combined bbox (e.g. multiple municipalities).\n\n\ncity_pr\nNo\nProvince or region hint for geocoding when needed.\n\n\n\nAll maps on this page were produced with the same pipeline; only these parameters change. Please note that because so many maps were generated at once, OSM may have imposed rate limits on these maps and generated incorrect values. Under normal circumstances, OSM will not rate limit maps generated with this tool."
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/guide.html",
    "href": "projects/Leaflet_Mapping_Tool/guide.html",
    "title": "Guide",
    "section": "",
    "text": "This tool builds an interactive bivariate map: Income (median household income by DA) × green area per 1,000 people, with greenspace from OpenStreetMap. Run from the project root."
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/guide.html#overview",
    "href": "projects/Leaflet_Mapping_Tool/guide.html#overview",
    "title": "Guide",
    "section": "",
    "text": "This tool builds an interactive bivariate map: Income (median household income by DA) × green area per 1,000 people, with greenspace from OpenStreetMap. Run from the project root."
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/guide.html#one-time-setup",
    "href": "projects/Leaflet_Mapping_Tool/guide.html#one-time-setup",
    "title": "Guide",
    "section": "One-time setup",
    "text": "One-time setup\n\n1. Python: Fetch census and boundaries\nFrom the project root:\npip install -r requirements.txt\npython scripts/fetch_statcan_census.py --all\nThis downloads 2021 DA boundaries and one Census Profile region (Atlantic). For all of Canada, run:\npython scripts/fetch_statcan_census.py --boundaries --all-regions\n\n\n2. R: Build income/SES GeoPackage\nIn R (with working directory = project root):\nsource(\"R/build_ses_index.R\")\nThis reads data/raw/ (census CSV and da_boundaries/), joins to DAs, and writes data/ses_index.gpkg. That file is used as income_gpkg when creating maps."
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/guide.html#creating-a-map",
    "href": "projects/Leaflet_Mapping_Tool/guide.html#creating-a-map",
    "title": "Guide",
    "section": "Creating a map",
    "text": "Creating a map\nIn R, use create_bivariate_map (Income × green area per 1000 people):\nsource(\"R/create_bivariate_map.R\")\n\n# Example: one city\ncreate_bivariate_map(\n  city = \"Halifax, NS\",\n  income_gpkg = \"data/ses_index.gpkg\"\n)\n\n# With expanded bbox (km)\ncreate_bivariate_map(\n  city = \"Hamilton, ON\",\n  income_gpkg = \"data/ses_index.gpkg\",\n  custom_distance_km = 10\n)\n\n# Custom bounding box\ncreate_bivariate_map(\n  city = \"Brandon, MB\",\n  income_gpkg = \"data/ses_index.gpkg\",\n  custom_bbox = c(-100.07, 49.78, -99.82, 49.91)\n)\nMaps are saved under output/maps/&lt;City&gt;_bivariate.html."
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/guide.html#data-and-map",
    "href": "projects/Leaflet_Mapping_Tool/guide.html#data-and-map",
    "title": "Guide",
    "section": "Data and map",
    "text": "Data and map\nbuild_ses_index.R uses 2021 Census variables at the DA level (education, income, employment) and writes a GeoPackage. The map displays median household income × green area per 1,000 people (Low/Med/High by study-area ±1 SD). Status categories in the build script (Lowest/…/Highest) come from R/map_config.R. See R/build_ses_index.R for details."
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/guide.html#data-sources",
    "href": "projects/Leaflet_Mapping_Tool/guide.html#data-sources",
    "title": "Guide",
    "section": "Data sources",
    "text": "Data sources\n\nCensus/income: Statistics Canada, 2021 Census of Population, Census Profile (DA-level).\nBoundaries: Statistics Canada, 2021 Dissemination area boundary file.\nGreen space: OpenStreetMap (parks, playgrounds, nature reserves)."
  },
  {
    "objectID": "projects/Leaflet_Mapping_Tool/guide.html#rendering-the-quarto-site",
    "href": "projects/Leaflet_Mapping_Tool/guide.html#rendering-the-quarto-site",
    "title": "Guide",
    "section": "Rendering the Quarto site",
    "text": "Rendering the Quarto site\nFrom the project root:\nquarto render\nThen open _site/index.html or publish the _site/ folder to your web host."
  },
  {
    "objectID": "projects/yoga-video-catalog/browse.html",
    "href": "projects/yoga-video-catalog/browse.html",
    "title": "Yoga catalog",
    "section": "",
    "text": "Filters\n      Clear all filters\n    \n    \n      \n        Duration\n        \n          Video length in minutes\n          \n            \n            \n              \n              \n              \n              \n              \n            \n            \n              \n                Minimum\n                \n                  \n                  min\n                \n              \n              \n                Maximum\n                \n                  \n                  min\n                \n              \n            \n          \n        \n        \n          Clear\n        \n      \n      \n      \n        Upload Year\n        \n          Note: Minimum and maximum dates are based on January 1\n          \n            \n            \n              \n              \n              \n              \n              \n            \n            \n              \n                Minimum\n                \n                  \n                  year\n                \n              \n              \n                Maximum\n                \n                  \n                  year\n                \n              \n            \n          \n        \n        \n          Clear\n        \n      \n    \n    \n      Focus\n      Click to include · double-click to exclude · triple-click to reset\n      \n        Clear\n      \n      \n    \n    \n      Channel\n      Click to include · double-click to exclude · triple-click to reset\n      \n        Clear\n      \n      \n    \n    \n      Search\n      \n        \n      \n      \n        Clear\n      \n    \n  \n  \n    \n      \n        My List\n        Drag to reorder. Click a title to open full-screen in a new tab.\n        0 video(s) · Total: —\n        \n      \n      \n        Clear all\n      \n    \n    \n  \n  \n    Videos\n    \n  \n  \n  No videos match your filters. Try clearing some filters or search."
  },
  {
    "objectID": "projects/example-other.html",
    "href": "projects/example-other.html",
    "title": "Example Other Project",
    "section": "",
    "text": "Placeholder: Short description of a miscellaneous project.\nThis is where you can add write-ups that don’t fit the other categories. Use the sidebar to navigate back to a category or the projects overview."
  },
  {
    "objectID": "projects/example-other.html#overview",
    "href": "projects/example-other.html#overview",
    "title": "Example Other Project",
    "section": "",
    "text": "Placeholder: Short description of a miscellaneous project.\nThis is where you can add write-ups that don’t fit the other categories. Use the sidebar to navigate back to a category or the projects overview."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Projects\nPlaceholder content."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hello!👋 I’m Kevin - a data, spatial, and research professional with experience across municipal, federal, and academic public‑sector environments. My work focuses on helping organizations make sense of complex health, spatial, and administrative data through rigorous analysis, thoughtful research design, and clear, user‑focused communication."
  },
  {
    "objectID": "about.html#my-background",
    "href": "about.html#my-background",
    "title": "About Me",
    "section": "My Background",
    "text": "My Background\nI completed my PhD in Geography at McMaster University, where I specialized in medical and spatial analysis. Since then, I’ve worked across public‑sector teams supporting analytics, program monitoring, performance measurement, accessibility research, and data‑driven decision‑making. My experience spans health, housing, transportation, and user experience research, giving me a broad understanding of how data, people, and systems interact."
  },
  {
    "objectID": "about.html#how-i-work",
    "href": "about.html#how-i-work",
    "title": "About Me",
    "section": "How I Work",
    "text": "How I Work\nI enjoy working across the full lifecycle of analytical and research projects:\n\n\nidentifying problems and clarifying requirements\n\nassessing data quality and documenting assumptions\n\ndesigning and conducting analysis\n\ntranslating results into clear, actionable insights\n\nbuilding workflows that are reproducible, maintainable, and future‑proof\n\nI value clarity, rigor, and thoughtful communication. I care deeply about documentation, data quality, and helping teams adopt tools and practices that make their work more reliable and sustainable."
  },
  {
    "objectID": "about.html#what-motivates-me",
    "href": "about.html#what-motivates-me",
    "title": "About Me",
    "section": "What Motivates Me",
    "text": "What Motivates Me\nI’m driven by curiosity and a desire to understand how systems work. I enjoy tackling complex, interdisciplinary problems and helping teams navigate uncertainty with structure and clarity. I’m especially motivated by work that supports public impact, equity, and evidence‑informed decision‑making."
  },
  {
    "objectID": "about.html#outside-of-work",
    "href": "about.html#outside-of-work",
    "title": "About Me",
    "section": "Outside of Work",
    "text": "Outside of Work\nI enjoy hiking, climbing, gravel biking, and spending time outdoors. These interests shape my appreciation for environmental stewardship, accessibility, and community‑focused public service."
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About Me",
    "section": "Contact",
    "text": "Contact\nYou can reach me at:\nmackaykp@gmail.com"
  }
]